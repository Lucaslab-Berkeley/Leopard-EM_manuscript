{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing of CHX yeast cells for Leopard-EM manuscript\n",
    "\n",
    "This notebook walks through the methods used to generate the 2DTM data from CHX treated yeast cells published in the 2025 Leopard-EM manuscript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Match Template\n",
    "Match template jobs were ran on the 60S and 40S subunits.\n",
    "The pdb models that were used for those are the same as for the constrained search tutorial, and can be found at [10.5281/zenodo.15368246](10.5281/zenodo.15368246).\n",
    "These models of the SSU body and LSU were centred and aligned with respect to eachother as described in that tutorial.\n",
    "\n",
    "### Simulating the maps\n",
    "The pixel size was optimized according to the procedure described in the constrained search tutorial, which gave a pixel size of 1.059.\n",
    "This was used to simulate maps of both the SSU body and the LSU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume generated\n"
     ]
    }
   ],
   "source": [
    "from ttsim3d.models import Simulator, SimulatorConfig\n",
    "\n",
    "# Instantiate the configuration object\n",
    "sim_conf = SimulatorConfig(\n",
    "    voltage=300.0,  # in keV\n",
    "    apply_dose_weighting=True,\n",
    "    dose_start=0.0,  # in e-/A^2\n",
    "    dose_end=50.0,  # in e-/A^2\n",
    "    dose_filter_modify_signal=\"rel_diff\",\n",
    "    upsampling=-1,  # auto\n",
    "    mtf_reference=\"k3_300kV_FL2\",\n",
    ")\n",
    "\n",
    "# Instantiate the simulator\n",
    "sim = Simulator(\n",
    "    pdb_filepath=\"../models/60S_aligned_aligned_zero.pdb\",\n",
    "    pixel_spacing=1.059,  # Angstroms\n",
    "    volume_shape=(512, 512, 512),\n",
    "    center_atoms=False,\n",
    "    remove_hydrogens=True,\n",
    "    b_factor_scaling=0.5,\n",
    "    additional_b_factor=0,\n",
    "    simulator_config=sim_conf,\n",
    ")\n",
    "\n",
    "# Run the simulation\n",
    "volume = sim.run()\n",
    "print(\"Volume generated\")\n",
    "\n",
    "mrc_filepath = \"../maps/60S_map_px1.059_bscale0.5.mrc\"\n",
    "sim.export_to_mrc(mrc_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume generated\n"
     ]
    }
   ],
   "source": [
    "from ttsim3d.models import Simulator, SimulatorConfig\n",
    "\n",
    "# Instantiate the configuration object\n",
    "sim_conf = SimulatorConfig(\n",
    "    voltage=300.0,  # in keV\n",
    "    apply_dose_weighting=True,\n",
    "    dose_start=0.0,  # in e-/A^2\n",
    "    dose_end=50.0,  # in e-/A^2\n",
    "    dose_filter_modify_signal=\"rel_diff\",\n",
    "    upsampling=-1,  # auto\n",
    "    mtf_reference=\"k3_300kV_FL2\",\n",
    ")\n",
    "\n",
    "# Instantiate the simulator\n",
    "sim = Simulator(\n",
    "    pdb_filepath=\"../models/6q8y_SSU_no_head_aligned_aligned_zero.pdb\",\n",
    "    pixel_spacing=1.059,  # Angstroms\n",
    "    volume_shape=(512, 512, 512),\n",
    "    center_atoms=False,\n",
    "    remove_hydrogens=True,\n",
    "    b_factor_scaling=0.5,\n",
    "    additional_b_factor=0,\n",
    "    simulator_config=sim_conf,\n",
    ")\n",
    "\n",
    "# Run the simulation\n",
    "volume = sim.run()\n",
    "print(\"Volume generated\")\n",
    "\n",
    "mrc_filepath = \"../maps/SSU-body_map_px1.059_bscale0.5.mrc\"\n",
    "sim.export_to_mrc(mrc_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run match template on both LSU and SSU\n",
    "\n",
    "We ran match template over all micrographs on our cluster using the following configurations and run scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "\n",
       "computational_config:\n",
       "  gpu_ids:\n",
       "  - 0\n",
       "  - 1\n",
       "  - 2\n",
       "  - 3\n",
       "  - 4\n",
       "  - 5\n",
       "  - 6\n",
       "  - 7\n",
       "  num_cpus: 16\n",
       "defocus_search_config:\n",
       "  defocus_max: 1200.0\n",
       "  defocus_min: -1200.0\n",
       "  defocus_step: 200.0\n",
       "  enabled: true\n",
       "match_template_result:\n",
       "  allow_file_overwrite: true\n",
       "  correlation_average_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_60S/output_correlation_average.mrc\n",
       "  correlation_variance_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_60S/output_correlation_variance.mrc\n",
       "  mip_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_60S/output_mip.mrc\n",
       "  orientation_phi_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_60S/output_orientation_phi.mrc\n",
       "  orientation_psi_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_60S/output_orientation_psi.mrc\n",
       "  orientation_theta_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_60S/output_orientation_theta.mrc\n",
       "  relative_defocus_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_60S/output_relative_defocus.mrc\n",
       "  scaled_mip_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_60S/output_scaled_mip.mrc\n",
       "micrograph_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/all_mgraphs/\n",
       "optics_group:\n",
       "  label: micrograph_1\n",
       "  amplitude_contrast_ratio: 0.07\n",
       "  ctf_B_factor: 150.0\n",
       "  astigmatism_angle: 39.417260\n",
       "  defocus_u: 5978.758301\n",
       "  defocus_v: 5617.462402\n",
       "  phase_shift: 0.0\n",
       "  pixel_size: 1.059\n",
       "  spherical_aberration: 2.7\n",
       "  voltage: 300.0\n",
       "orientation_search_config:\n",
       "  psi_step: 1.5\n",
       "  base_grid_method: uniform\n",
       "  theta_step: 2.5\n",
       "preprocessing_filters:\n",
       "  bandpass_filter:\n",
       "    enabled: false\n",
       "    falloff: 0.05\n",
       "    high_freq_cutoff: 0.5\n",
       "    low_freq_cutoff: 0.0\n",
       "  whitening_filter:\n",
       "    enabled: true\n",
       "    do_power_spectrum: true\n",
       "    max_freq: 1.0\n",
       "    num_freq_bins: null\n",
       "template_volume_path: /global/scratch/users/jdickerson/2dtm_test_data/maps/60S_map_px1.059_bscale0.5_k3dqe.mrc\n",
       "  \n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Read the YAML file\n",
    "with open(\"match_template_config_60S_base_H100.yaml\") as file:\n",
    "    yaml_content = file.read()\n",
    "\n",
    "# Display as markdown code block\n",
    "display(Markdown(f\"```yaml\\n{yaml_content}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "\n",
       "computational_config:\n",
       "  gpu_ids:\n",
       "  - 0\n",
       "  - 1\n",
       "  - 2\n",
       "  - 3\n",
       "  - 4\n",
       "  - 5\n",
       "  - 6\n",
       "  - 7\n",
       "  num_cpus: 16\n",
       "defocus_search_config:\n",
       "  defocus_max: 1200.0\n",
       "  defocus_min: -1200.0\n",
       "  defocus_step: 200.0\n",
       "  enabled: true\n",
       "match_template_result:\n",
       "  allow_file_overwrite: true\n",
       "  correlation_average_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_40S-body/output_correlation_average.mrc\n",
       "  correlation_variance_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_40S-body/output_correlation_variance.mrc\n",
       "  mip_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_40S-body/output_mip.mrc\n",
       "  orientation_phi_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_40S-body/output_orientation_phi.mrc\n",
       "  orientation_psi_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_40S-body/output_orientation_psi.mrc\n",
       "  orientation_theta_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_40S-body/output_orientation_theta.mrc\n",
       "  relative_defocus_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_40S-body/output_relative_defocus.mrc\n",
       "  scaled_mip_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/results_match_tm_40S-body/output_scaled_mip.mrc\n",
       "micrograph_path: /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/all_mgraphs/\n",
       "optics_group:\n",
       "  label: micrograph_1\n",
       "  amplitude_contrast_ratio: 0.07\n",
       "  ctf_B_factor: 150.0\n",
       "  astigmatism_angle: 39.417260\n",
       "  defocus_u: 5978.758301\n",
       "  defocus_v: 5617.462402\n",
       "  phase_shift: 0.0\n",
       "  pixel_size: 0.936\n",
       "  spherical_aberration: 2.7\n",
       "  voltage: 300.0\n",
       "orientation_search_config:\n",
       "  psi_step: 1.5\n",
       "  base_grid_method: uniform\n",
       "  theta_step: 2.5\n",
       "preprocessing_filters:\n",
       "  bandpass_filter:\n",
       "    enabled: false\n",
       "    falloff: 0.05\n",
       "    high_freq_cutoff: 0.5\n",
       "    low_freq_cutoff: 0.0\n",
       "  whitening_filter:\n",
       "    enabled: true\n",
       "    do_power_spectrum: true\n",
       "    max_freq: 1.0\n",
       "    num_freq_bins: null\n",
       "template_volume_path: /global/scratch/users/jdickerson/2dtm_test_data/maps/SSU-body_map_px1.059_bscale0.5_k3dqe.mrc\n",
       "  \n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Read the YAML file\n",
    "with open(\"match_template_config_40S_base_H100.yaml\") as file:\n",
    "    yaml_content = file.read()\n",
    "\n",
    "# Display as markdown code block\n",
    "display(Markdown(f\"```yaml\\n{yaml_content}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=match_template\n",
    "#SBATCH --account=pc_lucaslab\n",
    "#SBATCH --partition=es1\n",
    "#SBATCH --qos=es_normal\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --output=/global/scratch/users/jdickerson/2dtm_test_data/CHX_data/logs/match_template_%A_%a.out\n",
    "#SBATCH --error=/global/scratch/users/jdickerson/2dtm_test_data/CHX_data/logs/match_template_%A_%a.err\n",
    "#SBATCH --time=72:00:00\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --gres=gpu:H100:8\n",
    "\n",
    "mkdir -p /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/logs\n",
    "# Load any necessary modules (adjust for your system)\n",
    "# Print current shell and environment before activation\n",
    "echo \"=== ENVIRONMENT BEFORE ACTIVATION ===\"\n",
    "echo \"Current shell: $SHELL\"\n",
    "echo \"Current conda environments:\"\n",
    "conda env list\n",
    "echo \"Current Python: $(which python)\"\n",
    "echo \"Current Python version: $(python --version 2>&1)\"\n",
    "echo \"======================================\"\n",
    "\n",
    "# Activate leopard-em conda environment \n",
    "echo \"=== ACTIVATING CONDA ENVIRONMENT ===\"\n",
    "source $(conda info --base)/etc/profile.d/conda.sh\n",
    "conda activate leopard-em\n",
    "ACTIVATION_STATUS=$?\n",
    "\n",
    "# Check if activation succeeded\n",
    "if [ $ACTIVATION_STATUS -ne 0 ]; then\n",
    "    echo \"ERROR: Failed to activate the leopard-em environment\"\n",
    "    echo \"Available environments:\"\n",
    "    conda env list\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Print environment details after activation\n",
    "echo \"=== ENVIRONMENT AFTER ACTIVATION ===\"\n",
    "echo \"Active conda environment: $CONDA_PREFIX\"\n",
    "echo \"Python interpreter: $(which python)\"\n",
    "echo \"Python version: $(python --version 2>&1)\"\n",
    "echo \"Conda packages in environment:\"\n",
    "conda list | grep -E 'program|leopard'\n",
    "echo \"======================================\"\n",
    "\n",
    "# Test if the programs module is importable\n",
    "echo \"=== TESTING PYTHON IMPORTS ===\"\n",
    "python -c \"\n",
    "try:\n",
    "    import programs.match_template\n",
    "    print('SUCCESS: programs.match_template module found')\n",
    "except ImportError as e:\n",
    "    print(f'ERROR: Unable to import programs.match_template: {e}')\n",
    "    import sys\n",
    "    print(f'Python path: {sys.path}')\n",
    "\"\n",
    "echo \"======================================\"\n",
    "\n",
    "# Activate leopard-em conda environment\n",
    "\n",
    "\n",
    "# Set up paths (update these paths for your project)\n",
    "PROJECT_DIR=\"/global/scratch/users/jdickerson/2dtm_test_data/CHX_data\"\n",
    "MICROGRAPHS_DIR=\"${PROJECT_DIR}/all_mgraphs\"\n",
    "OUTPUT_DIR=\"${PROJECT_DIR}/results_match_tm_60S_2\"\n",
    "CTFS_DIR=\"${PROJECT_DIR}/all_ctfs/defocus_list.txt\"\n",
    "TEMPLATE_YAML=\"${PROJECT_DIR}/match_template_config_60S_base_H100.yaml\"\n",
    "SCRIPT_PATH=\"${PROJECT_DIR}/process_all_ctf-list.py\"\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "mkdir -p ${OUTPUT_DIR}\n",
    "\n",
    "# Run the processing script\n",
    "python ${SCRIPT_PATH} \\\n",
    "  --micrographs-dir \"${MICROGRAPHS_DIR}\" \\\n",
    "  --template-yaml \"${TEMPLATE_YAML}\" \\\n",
    "  --defocus-list \"${CTFS_DIR}\" \\\n",
    "  --output-dir \"${OUTPUT_DIR}\" \\\n",
    "  --gpus \"0,1,2,3,4,5,6,7\" \\\n",
    "  --batch-size 8 \\\n",
    "  --pattern \"*.mrc\"\n",
    "\n",
    "echo \"All micrographs processed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=match_template\n",
    "#SBATCH --account=pc_lucaslab\n",
    "#SBATCH --partition=es1\n",
    "#SBATCH --qos=es_normal\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --output=/global/scratch/users/jdickerson/2dtm_test_data/CHX_data/logs/match_template_40S_%A_%a.out\n",
    "#SBATCH --error=/global/scratch/users/jdickerson/2dtm_test_data/CHX_data/logs/match_template_40S_%A_%a.err\n",
    "#SBATCH --time=72:00:00\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --gres=gpu:H100:8\n",
    "\n",
    "mkdir -p /global/scratch/users/jdickerson/2dtm_test_data/CHX_data/logs\n",
    "# Load any necessary modules (adjust for your system)\n",
    "# Print current shell and environment before activation\n",
    "echo \"=== ENVIRONMENT BEFORE ACTIVATION ===\"\n",
    "echo \"Current shell: $SHELL\"\n",
    "echo \"Current conda environments:\"\n",
    "conda env list\n",
    "echo \"Current Python: $(which python)\"\n",
    "echo \"Current Python version: $(python --version 2>&1)\"\n",
    "echo \"======================================\"\n",
    "\n",
    "# Activate leopard-em conda environment \n",
    "echo \"=== ACTIVATING CONDA ENVIRONMENT ===\"\n",
    "source $(conda info --base)/etc/profile.d/conda.sh\n",
    "conda activate leopard-em\n",
    "ACTIVATION_STATUS=$?\n",
    "\n",
    "# Check if activation succeeded\n",
    "if [ $ACTIVATION_STATUS -ne 0 ]; then\n",
    "    echo \"ERROR: Failed to activate the leopard-em environment\"\n",
    "    echo \"Available environments:\"\n",
    "    conda env list\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Print environment details after activation\n",
    "echo \"=== ENVIRONMENT AFTER ACTIVATION ===\"\n",
    "echo \"Active conda environment: $CONDA_PREFIX\"\n",
    "echo \"Python interpreter: $(which python)\"\n",
    "echo \"Python version: $(python --version 2>&1)\"\n",
    "echo \"Conda packages in environment:\"\n",
    "conda list | grep -E 'program|leopard'\n",
    "echo \"======================================\"\n",
    "\n",
    "# Test if the programs module is importable\n",
    "echo \"=== TESTING PYTHON IMPORTS ===\"\n",
    "python -c \"\n",
    "try:\n",
    "    import programs.match_template\n",
    "    print('SUCCESS: programs.match_template module found')\n",
    "except ImportError as e:\n",
    "    print(f'ERROR: Unable to import programs.match_template: {e}')\n",
    "    import sys\n",
    "    print(f'Python path: {sys.path}')\n",
    "\"\n",
    "echo \"======================================\"\n",
    "\n",
    "# Activate leopard-em conda environment\n",
    "\n",
    "\n",
    "# Set up paths (update these paths for your project)\n",
    "PROJECT_DIR=\"/global/scratch/users/jdickerson/2dtm_test_data/CHX_data\"\n",
    "MICROGRAPHS_DIR=\"${PROJECT_DIR}/all_mgraphs\"\n",
    "OUTPUT_DIR=\"${PROJECT_DIR}/results_match_tm_40S-body_2\"\n",
    "CTFS_DIR=\"${PROJECT_DIR}/all_ctfs/defocus_list.txt\"\n",
    "TEMPLATE_YAML=\"${PROJECT_DIR}/match_template_config_40S_base_H100.yaml\"\n",
    "SCRIPT_PATH=\"${PROJECT_DIR}/process_all_ctf-list.py\"\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "mkdir -p ${OUTPUT_DIR}\n",
    "\n",
    "# Run the processing script\n",
    "python ${SCRIPT_PATH} \\\n",
    "  --micrographs-dir \"${MICROGRAPHS_DIR}\" \\\n",
    "  --template-yaml \"${TEMPLATE_YAML}\" \\\n",
    "  --defocus-list \"${CTFS_DIR}\" \\\n",
    "  --output-dir \"${OUTPUT_DIR}\" \\\n",
    "  --gpus \"0,1,2,3,4,5,6,7\" \\\n",
    "  --batch-size 8 \\\n",
    "  --pattern \"*.mrc\"\n",
    "\n",
    "echo \"All micrographs processed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"process_all_ctf-list.py\"\"\"\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import pandas as pd\n",
    "from leopard_em.pydantic_models.managers import MatchTemplateManager\n",
    "\n",
    "def load_defocus_list(defocus_list_path):\n",
    "    \"\"\"Load CTF parameters from a pipe-delimited defocus list file.\"\"\"\n",
    "    # Read the defocus list file into a DataFrame with pipe delimiter\n",
    "    df = pd.read_csv(defocus_list_path, sep='|', comment='#', skipinitialspace=True, dtype=str)\n",
    "    \n",
    "    # Create a dictionary mapping micrograph filenames to defocus parameters\n",
    "    defocus_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        # Clean up column values to remove any leading/trailing whitespace\n",
    "        full_path = row[0].strip()  # First column contains full filename\n",
    "        micrograph_filename = os.path.basename(full_path)\n",
    "        defocus_1 = float(row[1].strip())  # Second column has defocus_u (in Angstroms)\n",
    "        defocus_2 = float(row[2].strip())  # Third column has defocus_v (in Angstroms)\n",
    "        astigmatism_angle = float(row[3].strip())  # Fourth column has astigmatism angle\n",
    "        \n",
    "        defocus_dict[micrograph_filename] = (defocus_1, defocus_2, astigmatism_angle)\n",
    "    \n",
    "    return defocus_dict\n",
    "\n",
    "def create_yaml_for_micrograph(template_yaml_path, micrograph_path, defocus_1, defocus_2, astigmatism_angle, output_path, gpu_ids):\n",
    "    \"\"\"Create a custom YAML file for a specific micrograph.\"\"\"\n",
    "    # Load the template YAML\n",
    "    with open(template_yaml_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    # Update the config with the specific micrograph info\n",
    "    config['micrograph_path'] = micrograph_path\n",
    "    config['optics_group']['defocus_u'] = defocus_1\n",
    "    config['optics_group']['defocus_v'] = defocus_2\n",
    "    config['optics_group']['astigmatism_angle'] = astigmatism_angle\n",
    "    \n",
    "    # Set GPU IDs\n",
    "    config['computational_config']['gpu_ids'] = gpu_ids\n",
    "    \n",
    "    # Update output paths to be in the all_results folder with micrograph-specific names\n",
    "    micrograph_basename = os.path.basename(micrograph_path).split('.')[0]\n",
    "    results_dir = os.path.dirname(output_path)\n",
    "    \n",
    "    for key in config['match_template_result']:\n",
    "        if key != 'allow_file_overwrite':\n",
    "            original_path = config['match_template_result'][key]\n",
    "            filename = os.path.basename(original_path)\n",
    "            new_filename = f\"{micrograph_basename}_{filename}\"\n",
    "            config['match_template_result'][key] = os.path.join(results_dir, new_filename)\n",
    "    \n",
    "    # Write the updated config to a new YAML file\n",
    "    with open(output_path, 'w') as file:\n",
    "        yaml.dump(config, file, default_flow_style=False)\n",
    "    \n",
    "    return config\n",
    "\n",
    "def process_micrograph(micrograph_path, template_yaml, defocus_dict, output_dir, gpu_ids, batch_size):\n",
    "    \"\"\"Process a single micrograph with match template using defocus info from dict.\"\"\"\n",
    "    # Get just the filename for looking up in the defocus dictionary\n",
    "    micrograph_filename = os.path.basename(micrograph_path)\n",
    "    \n",
    "    # Get defocus parameters from dictionary\n",
    "    if micrograph_filename not in defocus_dict:\n",
    "        print(f\"Defocus parameters not found for {micrograph_filename}\")\n",
    "        return False\n",
    "    \n",
    "    defocus_1, defocus_2, astigmatism_angle = defocus_dict[micrograph_filename]\n",
    "    \n",
    "    # Create custom YAML file for this micrograph\n",
    "    custom_yaml_path = os.path.join(output_dir, f\"{os.path.splitext(micrograph_filename)[0]}_config.yaml\")\n",
    "    create_yaml_for_micrograph(\n",
    "        template_yaml, \n",
    "        micrograph_path, \n",
    "        defocus_1, \n",
    "        defocus_2, \n",
    "        astigmatism_angle, \n",
    "        custom_yaml_path,\n",
    "        gpu_ids\n",
    "    )\n",
    "    \n",
    "    # Run match template with the custom config\n",
    "    try:\n",
    "        print(f\"Running match template for {micrograph_filename} with GPUs {gpu_ids}\")\n",
    "        mt_manager = MatchTemplateManager.from_yaml(custom_yaml_path)\n",
    "        mt_manager.run_match_template(batch_size)\n",
    "        \n",
    "        # Save results to CSV\n",
    "        df = mt_manager.results_to_dataframe()\n",
    "        csv_path = os.path.join(output_dir, f\"{os.path.splitext(micrograph_filename)[0]}_results.csv\")\n",
    "        df.to_csv(csv_path)\n",
    "        \n",
    "        print(f\"Successfully processed {micrograph_filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {micrograph_filename}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def is_already_processed(micrograph_path, output_dir):\n",
    "    \"\"\"Check if a micrograph has already been processed by looking for its results in output_dir.\"\"\"\n",
    "    micrograph_filename = os.path.basename(micrograph_path)\n",
    "    results_filename = f\"{os.path.splitext(micrograph_filename)[0]}_results.csv\"\n",
    "    results_path = os.path.join(output_dir, results_filename)\n",
    "    return os.path.exists(results_path)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Process multiple micrographs with match template')\n",
    "    parser.add_argument('--micrographs-dir', required=True, help='Directory containing micrograph files')\n",
    "    parser.add_argument('--template-yaml', required=True, help='Path to the template YAML configuration')\n",
    "    parser.add_argument('--defocus-list', required=True, help='Path to the defocus list file')\n",
    "    parser.add_argument('--output-dir', required=True, help='Directory to store results')\n",
    "    parser.add_argument('--gpus', default='0', help='Comma-separated list of GPU IDs to use')\n",
    "    parser.add_argument('--batch-size', type=int, default=8, help='Orientation batch size')\n",
    "    parser.add_argument('--pattern', default='*DWS.mrc', help='File pattern to match micrographs')\n",
    "    parser.add_argument('--start-idx', type=int, default=None, help='Start index for processing (optional)')\n",
    "    parser.add_argument('--end-idx', type=int, default=None, help='End index for processing (optional)')\n",
    "    parser.add_argument('--job-idx', type=int, default=None, help='Job index from SLURM array (optional)')\n",
    "    parser.add_argument('--jobs-per-array', type=int, default=None, help='Number of micrographs per array job (optional)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Make sure output directory exists\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert GPU IDs string to list of integers\n",
    "    gpu_ids = [int(gpu_id) for gpu_id in args.gpus.split(',')]\n",
    "    \n",
    "    # Load defocus parameters from the defocus list file\n",
    "    defocus_dict = load_defocus_list(args.defocus_list)\n",
    "    print(f\"Loaded defocus parameters for {len(defocus_dict)} micrographs\")\n",
    "    \n",
    "    # Get list of micrograph files\n",
    "    micrograph_pattern = os.path.join(args.micrographs_dir, args.pattern)\n",
    "    micrograph_files = sorted(glob.glob(micrograph_pattern))\n",
    "    \n",
    "    if not micrograph_files:\n",
    "        print(f\"No micrograph files found matching pattern {micrograph_pattern}\")\n",
    "        return 1\n",
    "    \n",
    "    print(f\"Found {len(micrograph_files)} micrograph files\")\n",
    "    \n",
    "    # Filter out micrographs that have already been processed\n",
    "    unprocessed_micrographs = []\n",
    "    for micrograph_file in micrograph_files:\n",
    "        if not is_already_processed(micrograph_file, args.output_dir):\n",
    "            unprocessed_micrographs.append(micrograph_file)\n",
    "    \n",
    "    print(f\"Found {len(unprocessed_micrographs)} unprocessed micrographs out of {len(micrograph_files)} total\")\n",
    "    \n",
    "    # Replace the full list with only unprocessed micrographs\n",
    "    micrograph_files = unprocessed_micrographs\n",
    "    \n",
    "    # Check if there are any micrographs to process\n",
    "    if not micrograph_files:\n",
    "        print(\"No unprocessed micrographs to process. Exiting.\")\n",
    "        return 0\n",
    "    \n",
    "    # Determine which micrographs to process\n",
    "    if args.job_idx is not None and args.jobs_per_array is not None:\n",
    "        # Calculate range for this job in the array\n",
    "        start_idx = (args.job_idx - 1) * args.jobs_per_array\n",
    "        end_idx = min(start_idx + args.jobs_per_array, len(micrograph_files))\n",
    "        micrograph_files = micrograph_files[start_idx:end_idx]\n",
    "        print(f\"Processing micrographs {start_idx+1}-{end_idx} out of {len(micrograph_files)}\")\n",
    "    elif args.start_idx is not None or args.end_idx is not None:\n",
    "        start_idx = args.start_idx if args.start_idx is not None else 0\n",
    "        end_idx = args.end_idx if args.end_idx is not None else len(micrograph_files)\n",
    "        micrograph_files = micrograph_files[start_idx:end_idx]\n",
    "        print(f\"Processing micrographs {start_idx+1}-{end_idx} out of {len(micrograph_files)}\")\n",
    "    \n",
    "    # Process each micrograph\n",
    "    successful = 0\n",
    "    for i, micrograph_file in enumerate(micrograph_files):\n",
    "        print(f\"Processing {i+1}/{len(micrograph_files)}: {os.path.basename(micrograph_file)}\")\n",
    "        if process_micrograph(\n",
    "            micrograph_file, \n",
    "            args.template_yaml, \n",
    "            defocus_dict, \n",
    "            args.output_dir, \n",
    "            gpu_ids, \n",
    "            args.batch_size\n",
    "        ):\n",
    "            successful += 1\n",
    "    \n",
    "    print(f\"Successfully processed {successful}/{len(micrograph_files)} micrographs\")\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine template for the LSU\n",
    "After running the match template, the results were copied to a local workstation for further proceessing.\n",
    "The first step was to update the file paths in the results files with the new paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 _results.csv files\n",
      "Processing: results_match_tm_60S_2/153_Sep13_11.46.34_261_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/153_Sep13_11.46.34_261_1_results.csv\n",
      "Processing: results_match_tm_60S_2/100_Sep12_18.37.00_181_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/100_Sep12_18.37.00_181_1_results.csv\n",
      "Processing: results_match_tm_60S_2/95_Sep12_18.17.26_207_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/95_Sep12_18.17.26_207_1_results.csv\n",
      "Processing: results_match_tm_60S_2/77_Sep12_15.54.58_165_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/77_Sep12_15.54.58_165_1_results.csv\n",
      "Processing: results_match_tm_60S_2/153_Sep13_11.48.36_263_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/153_Sep13_11.48.36_263_1_results.csv\n",
      "Processing: results_match_tm_60S_2/130_Sep13_11.13.01_219_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/130_Sep13_11.13.01_219_1_results.csv\n",
      "Processing: results_match_tm_60S_2/79_Sep12_16.03.49_167_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/79_Sep12_16.03.49_167_1_results.csv\n",
      "Processing: results_match_tm_60S_2/25_Sep12_11.40.05_145_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/25_Sep12_11.40.05_145_1_results.csv\n",
      "Processing: results_match_tm_60S_2/147_Sep13_11.27.34_253_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/147_Sep13_11.27.34_253_1_results.csv\n",
      "Processing: results_match_tm_60S_2/142_Sep13_11.01.22_241_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/142_Sep13_11.01.22_241_1_results.csv\n",
      "Processing: results_match_tm_60S_2/158_Sep13_13.03.40_267_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/158_Sep13_13.03.40_267_1_results.csv\n",
      "Processing: results_match_tm_60S_2/94_Sep12_18.09.52_203_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/94_Sep12_18.09.52_203_1_results.csv\n",
      "Processing: results_match_tm_60S_2/131_Sep13_10.52.39_221_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/131_Sep13_10.52.39_221_1_results.csv\n",
      "Processing: results_match_tm_60S_2/138_Sep13_11.05.15_233_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/138_Sep13_11.05.15_233_1_results.csv\n",
      "Processing: results_match_tm_60S_2/107_Sep12_18.57.58_187_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/107_Sep12_18.57.58_187_1_results.csv\n",
      "Processing: results_match_tm_60S_2/82_Sep12_16.22.05_175_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/82_Sep12_16.22.05_175_1_results.csv\n",
      "Processing: results_match_tm_60S_2/111_Sep12_19.09.17_193_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/111_Sep12_19.09.17_193_1_results.csv\n",
      "Processing: results_match_tm_60S_2/47_Sep12_13.10.33_156_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/47_Sep12_13.10.33_156_1_results.csv\n",
      "Processing: results_match_tm_60S_2/141_Sep13_10.57.04_239_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/141_Sep13_10.57.04_239_1_results.csv\n",
      "Processing: results_match_tm_60S_2/105_Sep12_18.54.50_185_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/105_Sep12_18.54.50_185_1_results.csv\n",
      "Processing: results_match_tm_60S_2/147_Sep13_11.25.08_251_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/147_Sep13_11.25.08_251_1_results.csv\n",
      "Processing: results_match_tm_60S_2/163_Sep13_13.21.12_279_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/163_Sep13_13.21.12_279_1_results.csv\n",
      "Processing: results_match_tm_60S_2/139_Sep13_11.07.13_235_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/139_Sep13_11.07.13_235_1_results.csv\n",
      "Processing: results_match_tm_60S_2/101_Sep12_18.40.04_183_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/101_Sep12_18.40.04_183_1_results.csv\n",
      "Processing: results_match_tm_60S_2/77_Sep12_15.49.11_163_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/77_Sep12_15.49.11_163_1_results.csv\n",
      "Processing: results_match_tm_60S_2/143_Sep13_11.17.52_243_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/143_Sep13_11.17.52_243_1_results.csv\n",
      "Processing: results_match_tm_60S_2/97_Sep12_18.28.38_211_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/97_Sep12_18.28.38_211_1_results.csv\n",
      "Processing: results_match_tm_60S_2/147_Sep13_11.30.28_255_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/147_Sep13_11.30.28_255_1_results.csv\n",
      "Processing: results_match_tm_60S_2/150_Sep13_11.35.16_257_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/150_Sep13_11.35.16_257_1_results.csv\n",
      "Processing: results_match_tm_60S_2/95_Sep12_18.14.14_205_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/95_Sep12_18.14.14_205_1_results.csv\n",
      "Processing: results_match_tm_60S_2/82_Sep12_16.17.55_173_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/82_Sep12_16.17.55_173_1_results.csv\n",
      "Processing: results_match_tm_60S_2/151_Sep13_11.42.25_259_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/151_Sep13_11.42.25_259_1_results.csv\n",
      "Processing: results_match_tm_60S_2/44_Sep12_12.54.40_150_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/44_Sep12_12.54.40_150_1_results.csv\n",
      "Processing: results_match_tm_60S_2/114_Sep12_19.17.31_199_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/114_Sep12_19.17.31_199_1_results.csv\n",
      "Processing: results_match_tm_60S_2/136_Sep13_10.27.07_231_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/136_Sep13_10.27.07_231_1_results.csv\n",
      "Processing: results_match_tm_60S_2/154_Sep13_11.39.01_265_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/154_Sep13_11.39.01_265_1_results.csv\n",
      "Processing: results_match_tm_60S_2/146_Sep13_10.20.11_249_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/146_Sep13_10.20.11_249_1_results.csv\n",
      "Processing: results_match_tm_60S_2/161_Sep13_13.14.49_275_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/161_Sep13_13.14.49_275_1_results.csv\n",
      "Processing: results_match_tm_60S_2/128_Sep13_10.30.35_215_2_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/128_Sep13_10.30.35_215_2_results.csv\n",
      "Processing: results_match_tm_60S_2/145_Sep13_10.16.47_247_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/145_Sep13_10.16.47_247_1_results.csv\n",
      "Processing: results_match_tm_60S_2/140_Sep13_11.09.36_237_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/140_Sep13_11.09.36_237_1_results.csv\n",
      "Processing: results_match_tm_60S_2/135_Sep13_10.23.29_229_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/135_Sep13_10.23.29_229_1_results.csv\n",
      "Processing: results_match_tm_60S_2/144_Sep13_11.20.47_245_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/144_Sep13_11.20.47_245_1_results.csv\n",
      "Processing: results_match_tm_60S_2/93_Sep12_18.05.02_201_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/93_Sep12_18.05.02_201_1_results.csv\n",
      "Processing: results_match_tm_60S_2/113_Sep12_19.20.31_197_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_60S_2/113_Sep12_19.20.31_197_1_results.csv\n",
      "Path replacement completed!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def replace_paths_in_csv(input_dir):\n",
    "    \"\"\"\n",
    "    Read all _results.csv files in the specified directory, replace path strings,\n",
    "    and write back to the same files.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Directory containing _results.csv files\n",
    "    \"\"\"\n",
    "    old_path = \"/global/scratch/users/jdickerson/2dtm_test_data/\"\n",
    "    new_path = \"/home/data/jdickerson/Leopard-EM_paper_data/\"\n",
    "    \n",
    "    # Find all _results.csv files in the specified directory\n",
    "    csv_files = glob.glob(os.path.join(input_dir, \"**/*_results.csv\"), recursive=True)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No _results.csv files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} _results.csv files\")\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        print(f\"Processing: {csv_file}\")\n",
    "        \n",
    "        try:\n",
    "            # Read the CSV file using pandas\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # Replace path strings in all columns\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object':  # Only process string columns\n",
    "                    df[col] = df[col].astype(str).str.replace(old_path, new_path)\n",
    "            \n",
    "            # Write the modified dataframe back to the file\n",
    "            df.to_csv(csv_file, index=False)\n",
    "            print(f\"  Successfully updated paths in {csv_file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {csv_file}: {e}\")\n",
    "    \n",
    "    print(\"Path replacement completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python replace_paths.py <directory_path>\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    input_dir = \"results_match_tm_60S_2/\"\n",
    "    \n",
    "    if not os.path.isdir(input_dir):\n",
    "        print(f\"Error: {input_dir} is not a valid directory\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    replace_paths_in_csv(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 _results.csv files\n",
      "Processing: results_match_tm_40S-body_2/153_Sep13_11.46.34_261_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/153_Sep13_11.46.34_261_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/100_Sep12_18.37.00_181_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/100_Sep12_18.37.00_181_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/95_Sep12_18.17.26_207_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/95_Sep12_18.17.26_207_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/77_Sep12_15.54.58_165_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/77_Sep12_15.54.58_165_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/153_Sep13_11.48.36_263_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/153_Sep13_11.48.36_263_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/130_Sep13_11.13.01_219_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/130_Sep13_11.13.01_219_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/79_Sep12_16.03.49_167_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/79_Sep12_16.03.49_167_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/25_Sep12_11.40.05_145_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/25_Sep12_11.40.05_145_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/147_Sep13_11.27.34_253_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/147_Sep13_11.27.34_253_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/142_Sep13_11.01.22_241_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/142_Sep13_11.01.22_241_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/158_Sep13_13.03.40_267_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/158_Sep13_13.03.40_267_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/94_Sep12_18.09.52_203_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/94_Sep12_18.09.52_203_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/131_Sep13_10.52.39_221_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/131_Sep13_10.52.39_221_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/138_Sep13_11.05.15_233_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/138_Sep13_11.05.15_233_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/107_Sep12_18.57.58_187_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/107_Sep12_18.57.58_187_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/82_Sep12_16.22.05_175_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/82_Sep12_16.22.05_175_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/111_Sep12_19.09.17_193_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/111_Sep12_19.09.17_193_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/47_Sep12_13.10.33_156_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/47_Sep12_13.10.33_156_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/141_Sep13_10.57.04_239_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/141_Sep13_10.57.04_239_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/105_Sep12_18.54.50_185_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/105_Sep12_18.54.50_185_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/147_Sep13_11.25.08_251_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/147_Sep13_11.25.08_251_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/163_Sep13_13.21.12_279_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/163_Sep13_13.21.12_279_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/139_Sep13_11.07.13_235_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/139_Sep13_11.07.13_235_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/101_Sep12_18.40.04_183_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/101_Sep12_18.40.04_183_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/77_Sep12_15.49.11_163_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/77_Sep12_15.49.11_163_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/143_Sep13_11.17.52_243_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/143_Sep13_11.17.52_243_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/97_Sep12_18.28.38_211_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/97_Sep12_18.28.38_211_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/147_Sep13_11.30.28_255_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/147_Sep13_11.30.28_255_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/150_Sep13_11.35.16_257_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/150_Sep13_11.35.16_257_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/95_Sep12_18.14.14_205_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/95_Sep12_18.14.14_205_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/82_Sep12_16.17.55_173_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/82_Sep12_16.17.55_173_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/151_Sep13_11.42.25_259_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/151_Sep13_11.42.25_259_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/44_Sep12_12.54.40_150_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/44_Sep12_12.54.40_150_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/114_Sep12_19.17.31_199_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/114_Sep12_19.17.31_199_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/136_Sep13_10.27.07_231_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/136_Sep13_10.27.07_231_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/154_Sep13_11.39.01_265_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/154_Sep13_11.39.01_265_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/146_Sep13_10.20.11_249_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/146_Sep13_10.20.11_249_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/161_Sep13_13.14.49_275_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/161_Sep13_13.14.49_275_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/128_Sep13_10.30.35_215_2_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/128_Sep13_10.30.35_215_2_results.csv\n",
      "Processing: results_match_tm_40S-body_2/145_Sep13_10.16.47_247_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/145_Sep13_10.16.47_247_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/140_Sep13_11.09.36_237_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/140_Sep13_11.09.36_237_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/135_Sep13_10.23.29_229_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/135_Sep13_10.23.29_229_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/144_Sep13_11.20.47_245_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/144_Sep13_11.20.47_245_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/93_Sep12_18.05.02_201_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/93_Sep12_18.05.02_201_1_results.csv\n",
      "Processing: results_match_tm_40S-body_2/113_Sep12_19.20.31_197_1_results.csv\n",
      "  Successfully updated paths in results_match_tm_40S-body_2/113_Sep12_19.20.31_197_1_results.csv\n",
      "Path replacement completed!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def replace_paths_in_csv(input_dir):\n",
    "    \"\"\"\n",
    "    Read all _results.csv files in the specified directory, replace path strings,\n",
    "    and write back to the same files.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Directory containing _results.csv files\n",
    "    \"\"\"\n",
    "    old_path = \"/global/scratch/users/jdickerson/2dtm_test_data/\"\n",
    "    new_path = \"/home/data/jdickerson/Leopard-EM_paper_data/\"\n",
    "    \n",
    "    # Find all _results.csv files in the specified directory\n",
    "    csv_files = glob.glob(os.path.join(input_dir, \"**/*_results.csv\"), recursive=True)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No _results.csv files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} _results.csv files\")\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        print(f\"Processing: {csv_file}\")\n",
    "        \n",
    "        try:\n",
    "            # Read the CSV file using pandas\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # Replace path strings in all columns\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object':  # Only process string columns\n",
    "                    df[col] = df[col].astype(str).str.replace(old_path, new_path)\n",
    "            \n",
    "            # Write the modified dataframe back to the file\n",
    "            df.to_csv(csv_file, index=False)\n",
    "            print(f\"  Successfully updated paths in {csv_file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {csv_file}: {e}\")\n",
    "    \n",
    "    print(\"Path replacement completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python replace_paths.py <directory_path>\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    input_dir = \"results_match_tm_40S-body_2/\"\n",
    "    \n",
    "    if not os.path.isdir(input_dir):\n",
    "        print(f\"Error: {input_dir} is not a valid directory\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    replace_paths_in_csv(input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now running refine template with the following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "###################################################\n",
       "### RefineTemplateManager configuration example ###\n",
       "###################################################\n",
       "# An example YAML configuration to modify.\n",
       "# Call `RefineTemplateManager.from_yaml(path)` to load this configuration.\n",
       "template_volume_path: /home/data/jdickerson/Leopard-EM_paper_data/maps/60S_map_px1.059_bscale0.5_k3dqe.mrc\n",
       "particle_stack:\n",
       "  df_path: /some/path/to/particles.csv  # Needs to be readable by pandas\n",
       "  extracted_box_size: [528, 528]\n",
       "  original_template_size: [512, 512]\n",
       "defocus_refinement_config:\n",
       "  enabled: true\n",
       "  defocus_max:  100.0  # in Angstroms, relative to \"best\" defocus value in particle stack dataframe\n",
       "  defocus_min: -100.0  # in Angstroms, relative to \"best\" defocus value in particle stack dataframe\n",
       "  defocus_step: 20.0   # in Angstroms\n",
       "orientation_refinement_config:\n",
       "  enabled: true\n",
       "  psi_step_coarse:     1.5   # in degrees\n",
       "  psi_step_fine:       0.1  # in degrees\n",
       "  theta_step_coarse: 2.5   # in degrees\n",
       "  theta_step_fine:   0.1  # in degrees\n",
       "pixel_size_refinement_config:\n",
       "  enabled: false\n",
       "  pixel_size_min: -0.005\n",
       "  pixel_size_max: 0.005\n",
       "  pixel_size_step: 0.001\n",
       "preprocessing_filters:\n",
       "  whitening_filter:\n",
       "    do_power_spectrum: true\n",
       "    enabled: true\n",
       "    max_freq: 1.0  # In terms of Nyquist frequency\n",
       "    num_freq_bins: null\n",
       "  bandpass_filter:\n",
       "    enabled: false\n",
       "    falloff: null\n",
       "    high_freq_cutoff: null\n",
       "    low_freq_cutoff: null\n",
       "computational_config:\n",
       "  gpu_ids: \n",
       "    - 0\n",
       "    - 1\n",
       "    - 2\n",
       "    - 3\n",
       "  num_cpus: 8\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Read the YAML file\n",
    "with open(\"refine_template_config_60S.yaml\") as file:\n",
    "    yaml_content = file.read()\n",
    "\n",
    "# Display as markdown code block\n",
    "display(Markdown(f\"```yaml\\n{yaml_content}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load any necessary modules (adjust for your system)\n",
    "# Print current shell and environment before activation\n",
    "echo \"=== ENVIRONMENT BEFORE ACTIVATION ===\"\n",
    "echo \"Current shell: $SHELL\"\n",
    "echo \"Current conda environments:\"\n",
    "conda env list\n",
    "echo \"Current Python: $(which python)\"\n",
    "echo \"Current Python version: $(python --version 2>&1)\"\n",
    "echo \"======================================\"\n",
    "\n",
    "# Activate leopard-em conda environment \n",
    "echo \"=== ACTIVATING CONDA ENVIRONMENT ===\"\n",
    "source $(conda info --base)/etc/profile.d/conda.sh\n",
    "conda activate leopard-em\n",
    "ACTIVATION_STATUS=$?\n",
    "\n",
    "# Check if activation succeeded\n",
    "if [ $ACTIVATION_STATUS -ne 0 ]; then\n",
    "    echo \"ERROR: Failed to activate the leopard-em environment\"\n",
    "    echo \"Available environments:\"\n",
    "    conda env list\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Print environment details after activation\n",
    "echo \"=== ENVIRONMENT AFTER ACTIVATION ===\"\n",
    "echo \"Active conda environment: $CONDA_PREFIX\"\n",
    "echo \"Python interpreter: $(which python)\"\n",
    "echo \"Python version: $(python --version 2>&1)\"\n",
    "echo \"Conda packages in environment:\"\n",
    "conda list | grep -E 'program|leopard'\n",
    "echo \"======================================\"\n",
    "\n",
    "\n",
    "# Activate leopard-em conda environment\n",
    "\n",
    "\n",
    "# Set up paths (update these paths for your project)\n",
    "PROJECT_DIR=\"/home/data/jdickerson/Leopard-EM_paper_data/CHX_data\"\n",
    "MICROGRAPHS_DIR=\"${PROJECT_DIR}/all_mgraphs\"\n",
    "OUTPUT_DIR=\"${PROJECT_DIR}/results_refine_tm_60S\"\n",
    "MATCH_RESULTS_DIR=\"${PROJECT_DIR}/results_match_tm_60S\"\n",
    "TEMPLATE_YAML=\"${PROJECT_DIR}/refine_template_config_60S.yaml\"\n",
    "TEMPLATE_DIR=\"/home/data/jdickerson/Leopard-EM_paper_data/maps/60S_map_px1.059_bscale0.5_k3dqe.mrc\"\n",
    "RESULTS_SUFFIX=\"_results.csv\"\n",
    "SCRIPT_PATH=\"${PROJECT_DIR}/process_all_micrographs_refine.py\"\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "mkdir -p ${OUTPUT_DIR}\n",
    "\n",
    "# Run the processing script\n",
    "python ${SCRIPT_PATH} \\\n",
    "  --micrographs-dir \"${MICROGRAPHS_DIR}\" \\\n",
    "  --template-yaml \"${TEMPLATE_YAML}\" \\\n",
    "  --match-results-dir \"${MATCH_RESULTS_DIR}\" \\\n",
    "  --template-volume \"${TEMPLATE_DIR}\" \\\n",
    "  --output-dir \"${OUTPUT_DIR}\" \\\n",
    "  --results-suffix \"${RESULTS_SUFFIX}\" \\\n",
    "  --gpus \"0,1,2,3\" \\\n",
    "  --batch-size 64 \\\n",
    "  --pattern \"*.mrc\"\n",
    "\n",
    "echo \"All micrographs processed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "from programs.refine_template import RefineTemplateManager\n",
    "\n",
    "def extract_micrograph_number(filename):\n",
    "    \"\"\"Extract the micrograph number from the filename.\"\"\"\n",
    "    match = re.search(r'xenon_(\\d+)_(\\d+)_', filename)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}_{match.group(2)}\"\n",
    "    return None\n",
    "\n",
    "def create_yaml_for_refinement(template_yaml_path, match_results_csv, output_yaml_path, template_volume_path, gpu_ids):\n",
    "    \"\"\"Create a custom YAML file for refinement of a specific micrograph's match results.\"\"\"\n",
    "    # Load the template YAML\n",
    "    with open(template_yaml_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    # Update the config with the specific match results info\n",
    "    config['template_volume_path'] = template_volume_path\n",
    "    config['particle_stack']['df_path'] = match_results_csv\n",
    "    \n",
    "    # Set GPU IDs\n",
    "    config['computational_config']['gpu_ids'] = gpu_ids\n",
    "    \n",
    "    # Write the updated config to a new YAML file\n",
    "    with open(output_yaml_path, 'w') as file:\n",
    "        yaml.dump(config, file, default_flow_style=False)\n",
    "    \n",
    "    return config\n",
    "\n",
    "def process_micrograph_refinement(micrograph_path, match_results_csv, template_yaml, output_dir, template_volume_path, gpu_ids, batch_size):\n",
    "    \"\"\"Process refinement for a single micrograph match template results.\"\"\"\n",
    "    micrograph_basename = os.path.basename(micrograph_path)\n",
    "    micrograph_number = extract_micrograph_number(micrograph_basename)\n",
    "    \n",
    "    if not micrograph_number:\n",
    "        print(f\"Could not extract micrograph number from {micrograph_basename}\")\n",
    "        return False\n",
    "    \n",
    "    # Check if match results CSV exists\n",
    "    if not os.path.exists(match_results_csv):\n",
    "        print(f\"Match template results not found: {match_results_csv}\")\n",
    "        return False\n",
    "    \n",
    "    # Check if there are any results to refine\n",
    "    df = pd.read_csv(match_results_csv)\n",
    "    if len(df) == 0:\n",
    "        print(f\"No matches to refine in {match_results_csv}\")\n",
    "        return False\n",
    "    \n",
    "    # Create custom YAML file for refinement of this micrograph's results\n",
    "    custom_yaml_path = os.path.join(output_dir, f\"{os.path.splitext(micrograph_basename)[0]}_refine_config.yaml\")\n",
    "    create_yaml_for_refinement(\n",
    "        template_yaml, \n",
    "        match_results_csv, \n",
    "        custom_yaml_path,\n",
    "        template_volume_path,\n",
    "        gpu_ids\n",
    "    )\n",
    "    \n",
    "    # Run refine template with the custom config\n",
    "    try:\n",
    "        print(f\"Running refine template for {micrograph_basename} results with GPUs {gpu_ids}\")\n",
    "        rt_manager = RefineTemplateManager.from_yaml(custom_yaml_path)\n",
    "        \n",
    "        # Define output path for refinement results\n",
    "        refine_output_csv = os.path.join(output_dir, f\"{os.path.splitext(micrograph_basename)[0]}_refined_results.csv\")\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run refinement\n",
    "        rt_manager.run_refine_template(refine_output_csv, batch_size)\n",
    "        \n",
    "        # Calculate and print elapsed time\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "        print(f\"Refinement wall time: {elapsed_time_str}\")\n",
    "        \n",
    "        print(f\"Successfully refined matches for {micrograph_basename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error refining matches for {micrograph_basename}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Process multiple micrographs with refine template')\n",
    "    parser.add_argument('--micrographs-dir', required=True, help='Directory containing micrograph files')\n",
    "    parser.add_argument('--template-yaml', required=True, help='Path to the template refine YAML configuration')\n",
    "    parser.add_argument('--match-results-dir', required=True, help='Directory containing match template results')\n",
    "    parser.add_argument('--template-volume', required=True, help='Path to the template volume MRC file')\n",
    "    parser.add_argument('--output-dir', required=True, help='Directory to store refinement results')\n",
    "    parser.add_argument('--results-suffix', default='_results.csv', help='Suffix for match template results files (default: \"_results.csv\")')\n",
    "    parser.add_argument('--gpus', default='0', help='Comma-separated list of GPU IDs to use')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, help='Particle batch size for refinement')\n",
    "    parser.add_argument('--pattern', default='*DWS.mrc', help='File pattern to match micrographs')\n",
    "    parser.add_argument('--start-idx', type=int, default=None, help='Start index for processing (optional)')\n",
    "    parser.add_argument('--end-idx', type=int, default=None, help='End index for processing (optional)')\n",
    "    parser.add_argument('--job-idx', type=int, default=None, help='Job index from SLURM array (optional)')\n",
    "    parser.add_argument('--jobs-per-array', type=int, default=None, help='Number of micrographs per array job (optional)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Make sure output directory exists\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert GPU IDs string to list of integers\n",
    "    gpu_ids = [int(gpu_id) for gpu_id in args.gpus.split(',')]\n",
    "    \n",
    "    # Get list of micrograph files\n",
    "    micrograph_pattern = os.path.join(args.micrographs_dir, args.pattern)\n",
    "    micrograph_files = sorted(glob.glob(micrograph_pattern))\n",
    "    \n",
    "    if not micrograph_files:\n",
    "        print(f\"No micrograph files found matching pattern {micrograph_pattern}\")\n",
    "        return 1\n",
    "    \n",
    "    print(f\"Found {len(micrograph_files)} micrograph files\")\n",
    "    \n",
    "    # Determine which micrographs to process\n",
    "    if args.job_idx is not None and args.jobs_per_array is not None:\n",
    "        # Calculate range for this job in the array\n",
    "        start_idx = (args.job_idx - 1) * args.jobs_per_array\n",
    "        end_idx = min(start_idx + args.jobs_per_array, len(micrograph_files))\n",
    "        micrograph_files = micrograph_files[start_idx:end_idx]\n",
    "        print(f\"Processing micrographs {start_idx+1}-{end_idx} out of {len(micrograph_files)}\")\n",
    "    elif args.start_idx is not None or args.end_idx is not None:\n",
    "        start_idx = args.start_idx if args.start_idx is not None else 0\n",
    "        end_idx = args.end_idx if args.end_idx is not None else len(micrograph_files)\n",
    "        micrograph_files = micrograph_files[start_idx:end_idx]\n",
    "        print(f\"Processing micrographs {start_idx+1}-{end_idx} out of {len(micrograph_files)}\")\n",
    "    \n",
    "    # Process each micrograph's match results for refinement\n",
    "    successful = 0\n",
    "    for i, micrograph_file in enumerate(micrograph_files):\n",
    "        micrograph_basename = os.path.basename(micrograph_file)\n",
    "        base_name = os.path.splitext(micrograph_basename)[0]\n",
    "        \n",
    "        # Find the corresponding match results CSV file using the configurable suffix\n",
    "        match_results_csv = os.path.join(args.match_results_dir, f\"{base_name}{args.results_suffix}\")\n",
    "        \n",
    "        print(f\"Processing {i+1}/{len(micrograph_files)}: {micrograph_basename}\")\n",
    "        if process_micrograph_refinement(\n",
    "            micrograph_file, \n",
    "            match_results_csv,\n",
    "            args.template_yaml, \n",
    "            args.output_dir,\n",
    "            args.template_volume,\n",
    "            gpu_ids, \n",
    "            args.batch_size\n",
    "        ):\n",
    "            successful += 1\n",
    "    \n",
    "    print(f\"Successfully refined matches for {successful}/{len(micrograph_files)} micrographs\")\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing a constrained search for the SSU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constrained search follows the same 4 step process as for the untreated cells, except the angular ranges were optimized to be different.\n",
    "We are again running all searches with a false positive rate of 1/200 LSUs.\n",
    "\n",
    "Step 1 us a search around the Z-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "###################################################\n",
       "#### Constrained Search configuration example #####\n",
       "###################################################\n",
       "# An example YAML configuration to modify.\n",
       "# Call `RefineTemplateManager.from_yaml(path)` to load this configuration.\n",
       "template_volume_path: /home/data/jdickerson/Leopard-EM_paper_data/maps/SSU-body_map_px1.059_bscale0.5_k3dqe.mrc # Volume of small particle\n",
       "particle_stack_reference: # This is from the large particles\n",
       "  df_path: /some/path/to/particles.csv  # Needs to be readable by pandas\n",
       "  extracted_box_size: [520, 520]\n",
       "  original_template_size: [512, 512]\n",
       "particle_stack_constrained: # This is from the small particles\n",
       "  df_path: /some/path/to/particles.csv  # Needs to be readable by pandas\n",
       "  extracted_box_size: [520, 520]\n",
       "  original_template_size: [512, 512]\n",
       "centre_vector: [88.023109, 52.080261, 45.528008] # Vector from large particle to small particle in Angstroms\n",
       "orientation_refinement_config:\n",
       "  enabled: true\n",
       "  base_grid_method: uniform \n",
       "  psi_step: 1.0   # psi in degrees\n",
       "  theta_step: 1.0   # theta and phi in degrees\n",
       "  rotation_axis_euler_angles: [76.2, 60.02, 0.0] # This is the rotation axis\n",
       "  phi_min: 0.0\n",
       "  phi_max: 0.0\n",
       "  theta_min: 0.0\n",
       "  theta_max: 0.0\n",
       "  psi_min: -13.0\n",
       "  psi_max: 2.5\n",
       "  search_roll_axis: false\n",
       "  roll_axis: [0.0,1.0] # [x,y] This defines the roll axis (orthogonal to the rotation axis). None means search\n",
       "  roll_step: 2.0 \n",
       "defocus_refinement_config:\n",
       "  enabled: false\n",
       "  defocus_max:  100.0  # in Angstroms, relative to \"best\" defocus value in particle stack dataframe\n",
       "  defocus_min: -100.0  # in Angstroms, relative to \"best\" defocus value in particle stack dataframe\n",
       "  defocus_step: 20.0   # in Angstroms\n",
       "preprocessing_filters:\n",
       "  whitening_filter:\n",
       "    do_power_spectrum: true\n",
       "    enabled: true\n",
       "    max_freq: 1.0  # In terms of Nyquist frequency\n",
       "    num_freq_bins: null\n",
       "  bandpass_filter:\n",
       "    enabled: false\n",
       "    falloff: null\n",
       "    high_freq_cutoff: null\n",
       "    low_freq_cutoff: null\n",
       "computational_config:\n",
       "  gpu_ids: \n",
       "    - 0\n",
       "    - 1\n",
       "    - 2\n",
       "    - 3\n",
       "  num_cpus: 8\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Read the YAML file\n",
    "with open(\"configs/constrained_search_config_SSU-body_step1.yaml\") as file:\n",
    "    yaml_content = file.read()\n",
    "\n",
    "# Display as markdown code block\n",
    "display(Markdown(f\"```yaml\\n{yaml_content}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Create logs directory\n",
    "mkdir -p logs\n",
    "\n",
    "# Print current shell and environment before activation\n",
    "echo \"=== ENVIRONMENT BEFORE ACTIVATION ===\"\n",
    "echo \"Current shell: $SHELL\"\n",
    "echo \"Current conda environments:\"\n",
    "conda env list\n",
    "echo \"Current Python: $(which python)\"\n",
    "echo \"Current Python version: $(python --version 2>&1)\"\n",
    "echo \"======================================\"\n",
    "\n",
    "# Activate leopard-em conda environment \n",
    "echo \"=== ACTIVATING CONDA ENVIRONMENT ===\"\n",
    "source $(conda info --base)/etc/profile.d/conda.sh\n",
    "conda activate leopard-em\n",
    "ACTIVATION_STATUS=$?\n",
    "\n",
    "# Check if activation succeeded\n",
    "if [ $ACTIVATION_STATUS -ne 0 ]; then\n",
    "    echo \"ERROR: Failed to activate the leopard-em environment\"\n",
    "    echo \"Available environments:\"\n",
    "    conda env list\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Print environment details after activation\n",
    "echo \"=== ENVIRONMENT AFTER ACTIVATION ===\"\n",
    "echo \"Active conda environment: $CONDA_PREFIX\"\n",
    "echo \"Python interpreter: $(which python)\"\n",
    "echo \"Python version: $(python --version 2>&1)\"\n",
    "echo \"Conda packages in environment:\"\n",
    "conda list | grep -E 'program|leopard'\n",
    "echo \"======================================\"\n",
    "\n",
    "\n",
    "# Set up paths (update these paths for your project)\n",
    "PROJECT_DIR=\"/home/data/jdickerson/Leopard-EM_paper_data/CHX_data\"\n",
    "MICROGRAPHS_DIR=\"${PROJECT_DIR}/all_mgraphs\"\n",
    "OUTPUT_DIR=\"${PROJECT_DIR}/results_constrained_step1\"\n",
    "LARGE_RESULTS_DIR=\"${PROJECT_DIR}/results_refine_tm_60S\"\n",
    "SMALL_RESULTS_DIR=\"${PROJECT_DIR}/results_match_tm_40S-body\"\n",
    "TEMPLATE_YAML=\"${PROJECT_DIR}/configs/constrained_search_config_SSU-body_step1.yaml\"\n",
    "TEMPLATE_DIR=\"/home/data/jdickerson/Leopard-EM_paper_data/maps/SSU-body_map_px1.059_bscale0.5_k3dqe.mrc\"\n",
    "LARGE_SUFFIX=\"_refined_results.csv\"\n",
    "SMALL_SUFFIX=\"_results.csv\"\n",
    "SCRIPT_PATH=\"${PROJECT_DIR}/process_all_micrographs_constrained.py\"\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "mkdir -p ${OUTPUT_DIR}\n",
    "\n",
    "# Run the processing script\n",
    "python ${SCRIPT_PATH} \\\n",
    "  --micrographs-dir \"${MICROGRAPHS_DIR}\" \\\n",
    "  --template-yaml \"${TEMPLATE_YAML}\" \\\n",
    "  --large-results-dir \"${LARGE_RESULTS_DIR}\" \\\n",
    "  --small-results-dir \"${SMALL_RESULTS_DIR}\" \\\n",
    "  --template-volume \"${TEMPLATE_DIR}\" \\\n",
    "  --output-dir \"${OUTPUT_DIR}\" \\\n",
    "  --large-suffix \"${LARGE_SUFFIX}\" \\\n",
    "  --small-suffix \"${SMALL_SUFFIX}\" \\\n",
    "  --gpus \"0,1\" \\\n",
    "  --batch-size 64 \\\n",
    "  --false-positives 0.005\n",
    "\n",
    "echo \"All micrographs processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "from leopard_em.pydantic_models.managers import ConstrainedSearchManager\n",
    "\n",
    "def extract_micrograph_number(filename):\n",
    "    \"\"\"Extract the micrograph number from the filename.\"\"\"\n",
    "    # The new pattern matches filenames like: 25_Sep12_11.40.05_145_1.mrc\n",
    "    # Simply returns the base filename without extension\n",
    "    # This makes it work directly with similar result files: 25_Sep12_11.40.05_145_1_results.csv\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    return base_name\n",
    "\n",
    "def create_yaml_for_constrained_search(template_yaml_path, large_results_csv, small_results_csv, output_yaml_path, template_volume_path, gpu_ids):\n",
    "    \"\"\"Create a custom YAML file for constrained search of a specific micrograph's match results.\"\"\"\n",
    "    # Load the template YAML\n",
    "    with open(template_yaml_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    # Update the config with the specific match results info\n",
    "    config['template_volume_path'] = template_volume_path\n",
    "    config['particle_stack_reference']['df_path'] = large_results_csv\n",
    "    config['particle_stack_constrained']['df_path'] = small_results_csv\n",
    "    \n",
    "    # Set GPU IDs\n",
    "    config['computational_config']['gpu_ids'] = gpu_ids\n",
    "    \n",
    "    # Write the updated config to a new YAML file\n",
    "    with open(output_yaml_path, 'w') as file:\n",
    "        yaml.dump(config, file, default_flow_style=False)\n",
    "    \n",
    "    return config\n",
    "\n",
    "def process_micrograph_constrained_search(micrograph_path, large_results_csv, small_results_csv, template_yaml, output_dir, template_volume_path, gpu_ids, batch_size, false_positives):\n",
    "    \"\"\"Process constrained search for a single micrograph's match template results.\"\"\"\n",
    "    micrograph_basename = os.path.basename(micrograph_path)\n",
    "    base_name = os.path.splitext(micrograph_basename)[0]\n",
    "    \n",
    "    # Check if match results CSV files exist\n",
    "    if not os.path.exists(large_results_csv):\n",
    "        print(f\"Large particle results not found: {large_results_csv}\")\n",
    "        return False\n",
    "    \n",
    "    if not os.path.exists(small_results_csv):\n",
    "        print(f\"Small particle results not found: {small_results_csv}\")\n",
    "        return False\n",
    "    \n",
    "    # Check if there are any results to process\n",
    "    df_large = pd.read_csv(large_results_csv)\n",
    "    if len(df_large) == 0:\n",
    "        print(f\"No large particle matches in {large_results_csv}\")\n",
    "        return False\n",
    "    \n",
    "    df_small = pd.read_csv(small_results_csv)\n",
    "    if len(df_small) == 0:\n",
    "        print(f\"No small particle matches in {small_results_csv}\")\n",
    "        return False\n",
    "    \n",
    "    # Create custom YAML file for constrained search of this micrograph's results\n",
    "    custom_yaml_path = os.path.join(output_dir, f\"{base_name}_constrained_config.yaml\")\n",
    "    create_yaml_for_constrained_search(\n",
    "        template_yaml, \n",
    "        large_results_csv, \n",
    "        small_results_csv,\n",
    "        custom_yaml_path,\n",
    "        template_volume_path,\n",
    "        gpu_ids\n",
    "    )\n",
    "    \n",
    "    # Run constrained search with the custom config\n",
    "    try:\n",
    "        print(f\"Running constrained search for {micrograph_basename} with GPUs {gpu_ids}\")\n",
    "        cs_manager = ConstrainedSearchManager.from_yaml(custom_yaml_path)\n",
    "        \n",
    "        # Define output path for constrained search results\n",
    "        constrained_output_csv = os.path.join(output_dir, f\"{base_name}_constrained_results.csv\")\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run constrained search\n",
    "        cs_manager.run_constrained_search(constrained_output_csv, false_positives, batch_size)\n",
    "        \n",
    "        # Calculate and print elapsed time\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "        print(f\"Constrained search wall time: {elapsed_time_str}\")\n",
    "        \n",
    "        print(f\"Successfully completed constrained search for {micrograph_basename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error in constrained search for {micrograph_basename}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def is_already_processed(micrograph_path, output_dir):\n",
    "    \"\"\"Check if a micrograph has already been processed by looking for its constrained search results in output_dir.\"\"\"\n",
    "    micrograph_filename = os.path.basename(micrograph_path)\n",
    "    results_filename = f\"{os.path.splitext(micrograph_filename)[0]}_constrained_results.csv\"\n",
    "    results_path = os.path.join(output_dir, results_filename)\n",
    "    return os.path.exists(results_path)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Process multiple micrographs with constrained search')\n",
    "    parser.add_argument('--micrographs-dir', required=True, help='Directory containing micrograph files')\n",
    "    parser.add_argument('--template-yaml', required=True, help='Path to the template constrained search YAML configuration')\n",
    "    parser.add_argument('--large-results-dir', required=True, help='Directory containing large particle results')\n",
    "    parser.add_argument('--small-results-dir', required=True, help='Directory containing small particle results')\n",
    "    parser.add_argument('--template-volume', required=True, help='Path to the template volume MRC file (small particle)')\n",
    "    parser.add_argument('--output-dir', required=True, help='Directory to store constrained search results')\n",
    "    parser.add_argument('--large-suffix', default='_refined_results.csv', help='Suffix for large particle result files (default: \"_refined_results.csv\")')\n",
    "    parser.add_argument('--small-suffix', default='_results.csv', help='Suffix for small particle result files (default: \"_results.csv\")')\n",
    "    parser.add_argument('--gpus', default='0', help='Comma-separated list of GPU IDs to use')\n",
    "    parser.add_argument('--batch-size', type=int, default=80, help='Particle batch size for constrained search')\n",
    "    parser.add_argument('--pattern', default='*.mrc', help='File pattern to match micrographs')\n",
    "    parser.add_argument('--false-positives', type=float, default=0.005, help='False positives rate for constrained search')\n",
    "    parser.add_argument('--start-idx', type=int, default=None, help='Start index for processing (optional)')\n",
    "    parser.add_argument('--end-idx', type=int, default=None, help='End index for processing (optional)')\n",
    "    parser.add_argument('--job-idx', type=int, default=None, help='Job index from SLURM array (optional)')\n",
    "    parser.add_argument('--jobs-per-array', type=int, default=None, help='Number of micrographs per array job (optional)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Make sure output directory exists\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert GPU IDs string to list of integers\n",
    "    gpu_ids = [int(gpu_id) for gpu_id in args.gpus.split(',')]\n",
    "    \n",
    "    # Get list of micrograph files\n",
    "    micrograph_pattern = os.path.join(args.micrographs_dir, args.pattern)\n",
    "    micrograph_files = sorted(glob.glob(micrograph_pattern))\n",
    "    \n",
    "    if not micrograph_files:\n",
    "        print(f\"No micrograph files found matching pattern {micrograph_pattern}\")\n",
    "        return 1\n",
    "    \n",
    "    print(f\"Found {len(micrograph_files)} micrograph files\")\n",
    "    \n",
    "    # Filter out micrographs that have already been processed\n",
    "    unprocessed_micrographs = []\n",
    "    for micrograph_file in micrograph_files:\n",
    "        if not is_already_processed(micrograph_file, args.output_dir):\n",
    "            unprocessed_micrographs.append(micrograph_file)\n",
    "    \n",
    "    print(f\"Found {len(unprocessed_micrographs)} unprocessed micrographs out of {len(micrograph_files)} total\")\n",
    "    \n",
    "    # Replace the full list with only unprocessed micrographs\n",
    "    micrograph_files = unprocessed_micrographs\n",
    "    \n",
    "    # Check if there are any micrographs to process\n",
    "    if not micrograph_files:\n",
    "        print(\"No unprocessed micrographs to process. Exiting.\")\n",
    "        return 0\n",
    "    \n",
    "    # Determine which micrographs to process\n",
    "    if args.job_idx is not None and args.jobs_per_array is not None:\n",
    "        # Calculate range for this job in the array\n",
    "        start_idx = (args.job_idx - 1) * args.jobs_per_array\n",
    "        end_idx = min(start_idx + args.jobs_per_array, len(micrograph_files))\n",
    "        micrograph_files = micrograph_files[start_idx:end_idx]\n",
    "        print(f\"Processing micrographs {start_idx+1}-{end_idx} out of {len(micrograph_files)}\")\n",
    "    elif args.start_idx is not None or args.end_idx is not None:\n",
    "        start_idx = args.start_idx if args.start_idx is not None else 0\n",
    "        end_idx = args.end_idx if args.end_idx is not None else len(micrograph_files)\n",
    "        micrograph_files = micrograph_files[start_idx:end_idx]\n",
    "        print(f\"Processing micrographs {start_idx+1}-{end_idx} out of {len(micrograph_files)}\")\n",
    "    \n",
    "    # Process each micrograph's match results for constrained search\n",
    "    successful = 0\n",
    "    for i, micrograph_file in enumerate(micrograph_files):\n",
    "        micrograph_basename = os.path.basename(micrograph_file)\n",
    "        base_name = os.path.splitext(micrograph_basename)[0]\n",
    "        \n",
    "        # Find the corresponding results CSV files using the configurable suffixes\n",
    "        large_results_csv = os.path.join(args.large_results_dir, f\"{base_name}{args.large_suffix}\")\n",
    "        small_results_csv = os.path.join(args.small_results_dir, f\"{base_name}{args.small_suffix}\")\n",
    "        \n",
    "        print(f\"Processing {i+1}/{len(micrograph_files)}: {micrograph_basename}\")\n",
    "        if process_micrograph_constrained_search(\n",
    "            micrograph_file, \n",
    "            large_results_csv,\n",
    "            small_results_csv,\n",
    "            args.template_yaml, \n",
    "            args.output_dir,\n",
    "            args.template_volume,\n",
    "            gpu_ids, \n",
    "            args.batch_size,\n",
    "            args.false_positives\n",
    "        ):\n",
    "            successful += 1\n",
    "    \n",
    "    print(f\"Successfully completed constrained search for {successful}/{len(micrograph_files)} micrographs\")\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second search was around the y axis (theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "###################################################\n",
       "#### Constrained Search configuration example #####\n",
       "###################################################\n",
       "# An example YAML configuration to modify.\n",
       "# Call `RefineTemplateManager.from_yaml(path)` to load this configuration.\n",
       "template_volume_path: /home/data/jdickerson/Leopard-EM_paper_data/maps/SSU-body_map_px1.059_bscale0.5_k3dqe.mrc # Volume of small particle\n",
       "particle_stack_reference: # This is from the large particles\n",
       "  df_path: /some/path/to/particles.csv  # Needs to be readable by pandas\n",
       "  extracted_box_size: [520, 520]\n",
       "  original_template_size: [512, 512]\n",
       "particle_stack_constrained: # This is from the small particles\n",
       "  df_path: /some/path/to/particles.csv  # Needs to be readable by pandas\n",
       "  extracted_box_size: [520, 520]\n",
       "  original_template_size: [512, 512]\n",
       "centre_vector: [88.023109, 52.080261, 45.528008] # Vector from large particle to small particle in Angstroms\n",
       "orientation_refinement_config:\n",
       "  enabled: true\n",
       "  base_grid_method: uniform \n",
       "  psi_step: 1.0   # psi in degrees\n",
       "  theta_step: 1.0   # theta and phi in degrees\n",
       "  rotation_axis_euler_angles: [76.2, 60.02, 0.0] # This is the rotation axis\n",
       "  phi_min: 0.0\n",
       "  phi_max: 0.0\n",
       "  theta_min: -8.0\n",
       "  theta_max: 2.0\n",
       "  psi_min: 0.0\n",
       "  psi_max: 0.0\n",
       "  search_roll_axis: false\n",
       "  roll_axis: [0.0,1.0] # [x,y] This defines the roll axis (orthogonal to the rotation axis). None means search\n",
       "  roll_step: 2.0 \n",
       "defocus_refinement_config:\n",
       "  enabled: false\n",
       "  defocus_max:  100.0  # in Angstroms, relative to \"best\" defocus value in particle stack dataframe\n",
       "  defocus_min: -100.0  # in Angstroms, relative to \"best\" defocus value in particle stack dataframe\n",
       "  defocus_step: 20.0   # in Angstroms\n",
       "preprocessing_filters:\n",
       "  whitening_filter:\n",
       "    do_power_spectrum: true\n",
       "    enabled: true\n",
       "    max_freq: 1.0  # In terms of Nyquist frequency\n",
       "    num_freq_bins: null\n",
       "  bandpass_filter:\n",
       "    enabled: false\n",
       "    falloff: null\n",
       "    high_freq_cutoff: null\n",
       "    low_freq_cutoff: null\n",
       "computational_config:\n",
       "  gpu_ids: \n",
       "    - 0\n",
       "    - 1\n",
       "    - 2\n",
       "    - 3\n",
       "  num_cpus: 8\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Read the YAML file\n",
    "with open(\"configs/constrained_search_config_SSU-body_step3.yaml\") as file:\n",
    "    yaml_content = file.read()\n",
    "\n",
    "# Display as markdown code block\n",
    "display(Markdown(f\"```yaml\\n{yaml_content}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Create logs directory\n",
    "mkdir -p logs\n",
    "\n",
    "# Print current shell and environment before activation\n",
    "echo \"=== ENVIRONMENT BEFORE ACTIVATION ===\"\n",
    "echo \"Current shell: $SHELL\"\n",
    "echo \"Current conda environments:\"\n",
    "conda env list\n",
    "echo \"Current Python: $(which python)\"\n",
    "echo \"Current Python version: $(python --version 2>&1)\"\n",
    "echo \"======================================\"\n",
    "\n",
    "# Activate leopard-em conda environment \n",
    "echo \"=== ACTIVATING CONDA ENVIRONMENT ===\"\n",
    "source $(conda info --base)/etc/profile.d/conda.sh\n",
    "conda activate leopard-em\n",
    "ACTIVATION_STATUS=$?\n",
    "\n",
    "# Check if activation succeeded\n",
    "if [ $ACTIVATION_STATUS -ne 0 ]; then\n",
    "    echo \"ERROR: Failed to activate the leopard-em environment\"\n",
    "    echo \"Available environments:\"\n",
    "    conda env list\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Print environment details after activation\n",
    "echo \"=== ENVIRONMENT AFTER ACTIVATION ===\"\n",
    "echo \"Active conda environment: $CONDA_PREFIX\"\n",
    "echo \"Python interpreter: $(which python)\"\n",
    "echo \"Python version: $(python --version 2>&1)\"\n",
    "echo \"Conda packages in environment:\"\n",
    "conda list | grep -E 'program|leopard'\n",
    "echo \"======================================\"\n",
    "\n",
    "\n",
    "# Set up paths (update these paths for your project)\n",
    "PROJECT_DIR=\"/home/data/jdickerson/Leopard-EM_paper_data/CHX_data\"\n",
    "MICROGRAPHS_DIR=\"${PROJECT_DIR}/all_mgraphs\"\n",
    "OUTPUT_DIR=\"${PROJECT_DIR}/results_constrained_step3\"\n",
    "LARGE_RESULTS_DIR=\"${PROJECT_DIR}/results_constrained_step1\"\n",
    "SMALL_RESULTS_DIR=\"${PROJECT_DIR}/results_match_tm_40S-body\"\n",
    "TEMPLATE_YAML=\"${PROJECT_DIR}/configs/constrained_search_config_SSU-body_step3.yaml\"\n",
    "TEMPLATE_DIR=\"/home/data/jdickerson/Leopard-EM_paper_data/maps/SSU-body_map_px1.059_bscale0.5_k3dqe.mrc\"\n",
    "LARGE_SUFFIX=\"_constrained_results.csv\"\n",
    "SMALL_SUFFIX=\"_results.csv\"\n",
    "SCRIPT_PATH=\"${PROJECT_DIR}/process_all_micrographs_constrained.py\"\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "mkdir -p ${OUTPUT_DIR}\n",
    "\n",
    "# Run the processing script\n",
    "python ${SCRIPT_PATH} \\\n",
    "  --micrographs-dir \"${MICROGRAPHS_DIR}\" \\\n",
    "  --template-yaml \"${TEMPLATE_YAML}\" \\\n",
    "  --large-results-dir \"${LARGE_RESULTS_DIR}\" \\\n",
    "  --small-results-dir \"${SMALL_RESULTS_DIR}\" \\\n",
    "  --template-volume \"${TEMPLATE_DIR}\" \\\n",
    "  --output-dir \"${OUTPUT_DIR}\" \\\n",
    "  --large-suffix \"${LARGE_SUFFIX}\" \\\n",
    "  --small-suffix \"${SMALL_SUFFIX}\" \\\n",
    "  --gpus \"0,1\" \\\n",
    "  --batch-size 64 \\\n",
    "  --false-positives 0.005\n",
    "\n",
    "echo \"All micrographs processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was a finer search around both angles simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "###################################################\n",
       "#### Constrained Search configuration example #####\n",
       "###################################################\n",
       "# An example YAML configuration to modify.\n",
       "# Call `RefineTemplateManager.from_yaml(path)` to load this configuration.\n",
       "template_volume_path: /home/data/jdickerson/Leopard-EM_paper_data/maps/SSU-body_map_px1.059_bscale0.5_k3dqe.mrc # Volume of small particle\n",
       "particle_stack_reference: # This is from the large particles\n",
       "  df_path: /some/path/to/particles.csv  # Needs to be readable by pandas\n",
       "  extracted_box_size: [520, 520]\n",
       "  original_template_size: [512, 512]\n",
       "particle_stack_constrained: # This is from the small particles\n",
       "  df_path: /some/path/to/particles.csv  # Needs to be readable by pandas\n",
       "  extracted_box_size: [520, 520]\n",
       "  original_template_size: [512, 512]\n",
       "centre_vector: [88.023109, 52.080261, 45.528008] # Vector from large particle to small particle in Angstroms\n",
       "orientation_refinement_config:\n",
       "  enabled: true\n",
       "  base_grid_method: uniform \n",
       "  psi_step: 0.5   # psi in degrees\n",
       "  theta_step: 0.5   # theta and phi in degrees\n",
       "  rotation_axis_euler_angles: [76.2, 60.02, 0.0] # This is the rotation axis\n",
       "  phi_min: 0.0\n",
       "  phi_max: 0.0\n",
       "  theta_min: -5.0\n",
       "  theta_max: 5.0\n",
       "  psi_min: -5.0\n",
       "  psi_max: 5.0\n",
       "  search_roll_axis: false\n",
       "  roll_axis: [0.0,1.0] # [x,y] This defines the roll axis (orthogonal to the rotation axis). None means search\n",
       "  roll_step: 2.0 \n",
       "defocus_refinement_config:\n",
       "  enabled: false\n",
       "  defocus_max:  100.0  # in Angstroms, relative to \"best\" defocus value in particle stack dataframe\n",
       "  defocus_min: -100.0  # in Angstroms, relative to \"best\" defocus value in particle stack dataframe\n",
       "  defocus_step: 20.0   # in Angstroms\n",
       "preprocessing_filters:\n",
       "  whitening_filter:\n",
       "    do_power_spectrum: true\n",
       "    enabled: true\n",
       "    max_freq: 1.0  # In terms of Nyquist frequency\n",
       "    num_freq_bins: null\n",
       "  bandpass_filter:\n",
       "    enabled: false\n",
       "    falloff: null\n",
       "    high_freq_cutoff: null\n",
       "    low_freq_cutoff: null\n",
       "computational_config:\n",
       "  gpu_ids: \n",
       "    - 0\n",
       "    - 1\n",
       "    - 2\n",
       "    - 3\n",
       "  num_cpus: 8\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Read the YAML file\n",
    "with open(\"configs/constrained_search_config_SSU-body_step4.yaml\") as file:\n",
    "    yaml_content = file.read()\n",
    "\n",
    "# Display as markdown code block\n",
    "display(Markdown(f\"```yaml\\n{yaml_content}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Print current shell and environment before activation\n",
    "echo \"=== ENVIRONMENT BEFORE ACTIVATION ===\"\n",
    "echo \"Current shell: $SHELL\"\n",
    "echo \"Current conda environments:\"\n",
    "conda env list\n",
    "echo \"Current Python: $(which python)\"\n",
    "echo \"Current Python version: $(python --version 2>&1)\"\n",
    "echo \"======================================\"\n",
    "\n",
    "# Activate leopard-em conda environment \n",
    "echo \"=== ACTIVATING CONDA ENVIRONMENT ===\"\n",
    "source $(conda info --base)/etc/profile.d/conda.sh\n",
    "conda activate leopard-em\n",
    "ACTIVATION_STATUS=$?\n",
    "\n",
    "# Check if activation succeeded\n",
    "if [ $ACTIVATION_STATUS -ne 0 ]; then\n",
    "    echo \"ERROR: Failed to activate the leopard-em environment\"\n",
    "    echo \"Available environments:\"\n",
    "    conda env list\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Print environment details after activation\n",
    "echo \"=== ENVIRONMENT AFTER ACTIVATION ===\"\n",
    "echo \"Active conda environment: $CONDA_PREFIX\"\n",
    "echo \"Python interpreter: $(which python)\"\n",
    "echo \"Python version: $(python --version 2>&1)\"\n",
    "echo \"Conda packages in environment:\"\n",
    "conda list | grep -E 'program|leopard'\n",
    "echo \"======================================\"\n",
    "\n",
    "\n",
    "# Set up paths (update these paths for your project)\n",
    "PROJECT_DIR=\"/home/data/jdickerson/Leopard-EM_paper_data/CHX_data\"\n",
    "MICROGRAPHS_DIR=\"${PROJECT_DIR}/all_mgraphs\"\n",
    "OUTPUT_DIR=\"${PROJECT_DIR}/results_constrained_step4\"\n",
    "LARGE_RESULTS_DIR=\"${PROJECT_DIR}/results_constrained_step3\"\n",
    "SMALL_RESULTS_DIR=\"${PROJECT_DIR}/results_match_tm_40S-body\"\n",
    "TEMPLATE_YAML=\"${PROJECT_DIR}/configs/constrained_search_config_SSU-body_step4.yaml\"\n",
    "TEMPLATE_DIR=\"/home/data/jdickerson/Leopard-EM_paper_data/maps/SSU-body_map_px1.059_bscale0.5_k3dqe.mrc\"\n",
    "LARGE_SUFFIX=\"_constrained_results.csv\"\n",
    "SMALL_SUFFIX=\"_results.csv\"\n",
    "SCRIPT_PATH=\"${PROJECT_DIR}/process_all_micrographs_constrained.py\"\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "mkdir -p ${OUTPUT_DIR}\n",
    "\n",
    "# Run the processing script\n",
    "python ${SCRIPT_PATH} \\\n",
    "  --micrographs-dir \"${MICROGRAPHS_DIR}\" \\\n",
    "  --template-yaml \"${TEMPLATE_YAML}\" \\\n",
    "  --large-results-dir \"${LARGE_RESULTS_DIR}\" \\\n",
    "  --small-results-dir \"${SMALL_RESULTS_DIR}\" \\\n",
    "  --template-volume \"${TEMPLATE_DIR}\" \\\n",
    "  --output-dir \"${OUTPUT_DIR}\" \\\n",
    "  --large-suffix \"${LARGE_SUFFIX}\" \\\n",
    "  --small-suffix \"${SMALL_SUFFIX}\" \\\n",
    "  --gpus \"0,1\" \\\n",
    "  --batch-size 64 \\\n",
    "  --false-positives 0.005\n",
    "\n",
    "echo \"All micrographs processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final search is even finer with both angles and combined witha  defocus search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "###################################################\n",
       "#### Constrained Search configuration example #####\n",
       "###################################################\n",
       "# An example YAML configuration to modify.\n",
       "# Call `RefineTemplateManager.from_yaml(path)` to load this configuration.\n",
       "template_volume_path: /home/data/jdickerson/Leopard-EM_paper_data/maps/SSU-body_map_px1.059_bscale0.5_k3dqe.mrc # Volume of small particle\n",
       "particle_stack_reference: # This is from the large particles\n",
       "  df_path: /some/path/to/particles.csv  # Needs to be readable by pandas\n",
       "  extracted_box_size: [520, 520]\n",
       "  original_template_size: [512, 512]\n",
       "particle_stack_constrained: # This is from the small particles\n",
       "  df_path: /some/path/to/particles.csv  # Needs to be readable by pandas\n",
       "  extracted_box_size: [520, 520]\n",
       "  original_template_size: [512, 512]\n",
       "centre_vector: [88.023109, 52.080261, 45.528008] # Vector from large particle to small particle in Angstroms\n",
       "orientation_refinement_config:\n",
       "  enabled: true\n",
       "  base_grid_method: uniform \n",
       "  psi_step: 0.1   # psi in degrees\n",
       "  theta_step: 0.1   # theta and phi in degrees\n",
       "  rotation_axis_euler_angles: [76.2, 60.02, 0.0] # This is the rotation axis\n",
       "  phi_min: 0.0\n",
       "  phi_max: 0.0\n",
       "  theta_min: -0.5\n",
       "  theta_max: 0.5\n",
       "  psi_min: -0.5\n",
       "  psi_max: 0.5\n",
       "  search_roll_axis: false\n",
       "  roll_axis: [0.0,1.0] # [x,y] This defines the roll axis (orthogonal to the rotation axis). None means search\n",
       "  roll_step: 2.0 \n",
       "defocus_refinement_config:\n",
       "  enabled: true\n",
       "  defocus_max:  100.0  # in Angstroms, relative to \"best\" defocus value in particle stack dataframe\n",
       "  defocus_min: -100.0  # in Angstroms, relative to \"best\" defocus value in particle stack dataframe\n",
       "  defocus_step: 20.0   # in Angstroms\n",
       "preprocessing_filters:\n",
       "  whitening_filter:\n",
       "    do_power_spectrum: true\n",
       "    enabled: true\n",
       "    max_freq: 1.0  # In terms of Nyquist frequency\n",
       "    num_freq_bins: null\n",
       "  bandpass_filter:\n",
       "    enabled: false\n",
       "    falloff: null\n",
       "    high_freq_cutoff: null\n",
       "    low_freq_cutoff: null\n",
       "computational_config:\n",
       "  gpu_ids: \n",
       "    - 0\n",
       "    - 1\n",
       "    - 2\n",
       "    - 3\n",
       "  num_cpus: 8\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Read the YAML file\n",
    "with open(\"configs/constrained_search_config_SSU-body_step5.yaml\") as file:\n",
    "    yaml_content = file.read()\n",
    "\n",
    "# Display as markdown code block\n",
    "display(Markdown(f\"```yaml\\n{yaml_content}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Print current shell and environment before activation\n",
    "echo \"=== ENVIRONMENT BEFORE ACTIVATION ===\"\n",
    "echo \"Current shell: $SHELL\"\n",
    "echo \"Current conda environments:\"\n",
    "conda env list\n",
    "echo \"Current Python: $(which python)\"\n",
    "echo \"Current Python version: $(python --version 2>&1)\"\n",
    "echo \"======================================\"\n",
    "\n",
    "# Activate leopard-em conda environment \n",
    "echo \"=== ACTIVATING CONDA ENVIRONMENT ===\"\n",
    "source $(conda info --base)/etc/profile.d/conda.sh\n",
    "conda activate leopard-em\n",
    "ACTIVATION_STATUS=$?\n",
    "\n",
    "# Check if activation succeeded\n",
    "if [ $ACTIVATION_STATUS -ne 0 ]; then\n",
    "    echo \"ERROR: Failed to activate the leopard-em environment\"\n",
    "    echo \"Available environments:\"\n",
    "    conda env list\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Print environment details after activation\n",
    "echo \"=== ENVIRONMENT AFTER ACTIVATION ===\"\n",
    "echo \"Active conda environment: $CONDA_PREFIX\"\n",
    "echo \"Python interpreter: $(which python)\"\n",
    "echo \"Python version: $(python --version 2>&1)\"\n",
    "echo \"Conda packages in environment:\"\n",
    "conda list | grep -E 'program|leopard'\n",
    "echo \"======================================\"\n",
    "\n",
    "\n",
    "# Set up paths (update these paths for your project)\n",
    "PROJECT_DIR=\"/home/data/jdickerson/Leopard-EM_paper_data/CHX_data\"\n",
    "MICROGRAPHS_DIR=\"${PROJECT_DIR}/all_mgraphs\"\n",
    "OUTPUT_DIR=\"${PROJECT_DIR}/results_constrained_step5\"\n",
    "LARGE_RESULTS_DIR=\"${PROJECT_DIR}/results_constrained_step4\"\n",
    "SMALL_RESULTS_DIR=\"${PROJECT_DIR}/results_match_tm_40S-body\"\n",
    "TEMPLATE_YAML=\"${PROJECT_DIR}/configs/constrained_search_config_SSU-body_step5.yaml\"\n",
    "TEMPLATE_DIR=\"/home/data/jdickerson/Leopard-EM_paper_data/maps/SSU-body_map_px1.059_bscale0.5_k3dqe.mrc\"\n",
    "LARGE_SUFFIX=\"_constrained_results.csv\"\n",
    "SMALL_SUFFIX=\"_results.csv\"\n",
    "SCRIPT_PATH=\"${PROJECT_DIR}/process_all_micrographs_constrained.py\"\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "mkdir -p ${OUTPUT_DIR}\n",
    "\n",
    "# Run the processing script\n",
    "python ${SCRIPT_PATH} \\\n",
    "  --micrographs-dir \"${MICROGRAPHS_DIR}\" \\\n",
    "  --template-yaml \"${TEMPLATE_YAML}\" \\\n",
    "  --large-results-dir \"${LARGE_RESULTS_DIR}\" \\\n",
    "  --small-results-dir \"${SMALL_RESULTS_DIR}\" \\\n",
    "  --template-volume \"${TEMPLATE_DIR}\" \\\n",
    "  --output-dir \"${OUTPUT_DIR}\" \\\n",
    "  --large-suffix \"${LARGE_SUFFIX}\" \\\n",
    "  --small-suffix \"${SMALL_SUFFIX}\" \\\n",
    "  --gpus \"0,1\" \\\n",
    "  --batch-size 64 \\\n",
    "  --false-positives 0.005\n",
    "\n",
    "echo \"All micrographs processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now process the results sequentially to get the final output particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Step 1: results_constrained_step1\n",
      "  Found 188 results files\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  100_Sep12_18.37.00_181_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  153_Sep13_11.48.36_263_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 1296 correlations (total: 1296)\n",
      "  138_Sep13_11.05.15_233_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  144_Sep13_11.20.47_245_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  94_Sep12_18.09.52_203_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  95_Sep12_18.14.14_205_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  150_Sep13_11.35.16_257_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  95_Sep12_18.17.26_207_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  77_Sep12_15.49.11_163_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  105_Sep12_18.54.50_185_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  147_Sep13_11.30.28_255_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  154_Sep13_11.39.01_265_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  98_Sep12_18.32.40_213_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  153_Sep13_11.46.34_261_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  146_Sep13_10.20.11_249_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  147_Sep13_11.27.34_253_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  161_Sep13_13.14.49_275_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 1296 correlations (total: 1296)\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 52731 correlations (total: 54027)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 52731 correlations (total: 54027)\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 891 correlations (total: 54918)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 891 correlations (total: 54918)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 891 correlations (total: 54918)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 891 correlations (total: 54918)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 891 correlations (total: 54918)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 891 correlations (total: 54918)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 891 correlations (total: 54918)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 891 correlations (total: 54918)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 891 correlations (total: 54918)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 891 correlations (total: 54918)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 891 correlations (total: 54918)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 891 correlations (total: 54918)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 891 correlations (total: 54918)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 891 correlations (total: 54918)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 891 correlations (total: 54918)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 891 correlations (total: 54918)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 891 correlations (total: 54918)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 891 correlations (total: 54918)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 891 correlations (total: 54918)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 891 correlations (total: 54918)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 891 correlations (total: 54918)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 891 correlations (total: 54918)\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 14256 correlations (total: 69174)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 14256 correlations (total: 69174)\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 1701 correlations (total: 70875)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 1701 correlations (total: 70875)\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 2511 correlations (total: 73386)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 2511 correlations (total: 73386)\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  100_Sep12_18.37.00_181_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  153_Sep13_11.48.36_263_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 1296 correlations (total: 74682)\n",
      "  138_Sep13_11.05.15_233_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  144_Sep13_11.20.47_245_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  94_Sep12_18.09.52_203_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  95_Sep12_18.14.14_205_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  150_Sep13_11.35.16_257_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  95_Sep12_18.17.26_207_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  77_Sep12_15.49.11_163_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  105_Sep12_18.54.50_185_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  147_Sep13_11.30.28_255_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  154_Sep13_11.39.01_265_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  98_Sep12_18.32.40_213_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  153_Sep13_11.46.34_261_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  146_Sep13_10.20.11_249_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  147_Sep13_11.27.34_253_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  161_Sep13_13.14.49_275_1_constrained: Added 1296 correlations (total: 2592)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 1296 correlations (total: 74682)\n",
      "  Threshold for 158_Sep13_13.03.40_267_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 140_Sep13_11.09.36_237_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 113_Sep12_19.20.31_197_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 151_Sep13_11.42.25_259_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 107_Sep12_18.57.58_187_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 142_Sep13_11.01.22_241_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 79_Sep12_16.03.49_167_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 100_Sep12_18.37.00_181_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 163_Sep13_13.21.12_279_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 153_Sep13_11.48.36_263_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 141_Sep13_10.57.04_239_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 82_Sep12_16.17.55_173_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 128_Sep13_10.30.35_215_2_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 138_Sep13_11.05.15_233_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 144_Sep13_11.20.47_245_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 94_Sep12_18.09.52_203_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 95_Sep12_18.14.14_205_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 147_Sep13_11.25.08_251_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 139_Sep13_11.07.13_235_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 150_Sep13_11.35.16_257_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 143_Sep13_11.17.52_243_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 95_Sep12_18.17.26_207_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 77_Sep12_15.49.11_163_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 105_Sep12_18.54.50_185_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 97_Sep12_18.28.38_211_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 147_Sep13_11.30.28_255_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 154_Sep13_11.39.01_265_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 25_Sep12_11.40.05_145_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 111_Sep12_19.09.17_193_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 98_Sep12_18.32.40_213_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 153_Sep13_11.46.34_261_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 146_Sep13_10.20.11_249_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 82_Sep12_16.22.05_175_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 131_Sep13_10.52.39_221_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 147_Sep13_11.27.34_253_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 161_Sep13_13.14.49_275_1_constrained in step 1: 4.6189 (based on 2592 total correlations)\n",
      "  Threshold for 44_Sep12_12.54.40_150_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 135_Sep13_10.23.29_229_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "  Threshold for 93_Sep12_18.05.02_201_1_constrained in step 1: 5.2734 (based on 74682 total correlations)\n",
      "158_Sep13_13.03.40_267_1_constrained: 56 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 88 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 19 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 105 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 85 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 130 of 236 particles above threshold (using refined_scaled_mip)\n",
      "100_Sep12_18.37.00_181_1_constrained: 4 of 23 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 87 of 351 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.48.36_263_1_constrained: 131 of 406 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 114 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 20 of 212 particles above threshold (using refined_scaled_mip)\n",
      "138_Sep13_11.05.15_233_1_constrained: 211 of 661 particles above threshold (using refined_scaled_mip)\n",
      "144_Sep13_11.20.47_245_1_constrained: 125 of 311 particles above threshold (using refined_scaled_mip)\n",
      "94_Sep12_18.09.52_203_1_constrained: 128 of 314 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.14.14_205_1_constrained: 79 of 236 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 55 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 100 of 619 particles above threshold (using refined_scaled_mip)\n",
      "150_Sep13_11.35.16_257_1_constrained: 42 of 112 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 47 of 321 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.17.26_207_1_constrained: 121 of 405 particles above threshold (using refined_scaled_mip)\n",
      "77_Sep12_15.49.11_163_1_constrained: 135 of 197 particles above threshold (using refined_scaled_mip)\n",
      "105_Sep12_18.54.50_185_1_constrained: 74 of 189 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 45 of 398 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.30.28_255_1_constrained: 18 of 53 particles above threshold (using refined_scaled_mip)\n",
      "154_Sep13_11.39.01_265_1_constrained: 134 of 413 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 74 of 516 particles above threshold (using refined_scaled_mip)\n",
      "98_Sep12_18.32.40_213_1_constrained: 71 of 299 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.46.34_261_1_constrained: 84 of 354 particles above threshold (using refined_scaled_mip)\n",
      "146_Sep13_10.20.11_249_1_constrained: 14 of 68 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/82_Sep12_16.22.05_175_1_constrained_results.csv\n",
      "131_Sep13_10.52.39_221_1_constrained: 11 of 106 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.27.34_253_1_constrained: 106 of 257 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/161_Sep13_13.14.49_275_1_constrained_results.csv\n",
      "  No particles above threshold in results_constrained_step1/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 5 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 47 of 316 particles above threshold (using refined_scaled_mip)\n",
      "158_Sep13_13.03.40_267_1_constrained: 167 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 239 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 59 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 251 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_-8-2_step0.5/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 236 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 159 of 236 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 215 of 351 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 304 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 60 of 212 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 155 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 321 of 619 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 143 of 321 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 137 of 398 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_-8-2_step0.5/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 212 of 516 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.22.05_175_1_constrained: 1 of 29 particles above threshold (using refined_scaled_mip)\n",
      "131_Sep13_10.52.39_221_1_constrained: 35 of 106 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_-8-2_step0.5/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 17 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 145 of 316 particles above threshold (using refined_scaled_mip)\n",
      "158_Sep13_13.03.40_267_1_constrained: 9 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 11 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 3 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 15 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_0-0_theta_-8-2_step1/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 14 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 40 of 236 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 25 of 351 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 15 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 2 of 212 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 11 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 17 of 619 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 11 of 321 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 6 of 398 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_0-0_theta_-8-2_step1/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 11 of 516 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_0-0_theta_-8-2_step1/82_Sep12_16.22.05_175_1_constrained_results.csv\n",
      "131_Sep13_10.52.39_221_1_constrained: 2 of 106 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_0-0_theta_-8-2_step1/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 1 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 5 of 316 particles above threshold (using refined_scaled_mip)\n",
      "158_Sep13_13.03.40_267_1_constrained: 160 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 219 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 51 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 235 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_-8-2_step1/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 219 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 131 of 236 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 194 of 351 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 267 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 53 of 212 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 137 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 300 of 619 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 133 of 321 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 124 of 398 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_-8-2_step1/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 192 of 516 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.22.05_175_1_constrained: 1 of 29 particles above threshold (using refined_scaled_mip)\n",
      "131_Sep13_10.52.39_221_1_constrained: 33 of 106 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_-8-2_step1/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 15 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 128 of 316 particles above threshold (using refined_scaled_mip)\n",
      "158_Sep13_13.03.40_267_1_constrained: 9 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 11 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 3 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 16 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_0-0_theta_-8-2_step0.5/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 15 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 41 of 236 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 27 of 351 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 16 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 3 of 212 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 11 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 19 of 619 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 11 of 321 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 6 of 398 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_0-0_theta_-8-2_step0.5/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 11 of 516 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_0-0_theta_-8-2_step0.5/82_Sep12_16.22.05_175_1_constrained_results.csv\n",
      "131_Sep13_10.52.39_221_1_constrained: 2 of 106 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_0-0_theta_-8-2_step0.5/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 1 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 5 of 316 particles above threshold (using refined_scaled_mip)\n",
      "158_Sep13_13.03.40_267_1_constrained: 59 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 96 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 19 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 107 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_0-0_step0.5/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 90 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 133 of 236 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 92 of 351 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 123 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 23 of 212 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 56 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 109 of 619 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 51 of 321 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 49 of 398 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_0-0_step0.5/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 82 of 516 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_0-0_step0.5/82_Sep12_16.22.05_175_1_constrained_results.csv\n",
      "131_Sep13_10.52.39_221_1_constrained: 12 of 106 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_0-0_step0.5/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 6 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 55 of 316 particles above threshold (using refined_scaled_mip)\n",
      "158_Sep13_13.03.40_267_1_constrained: 59 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 91 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 17 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 109 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_0-0_step1/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 92 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 129 of 236 particles above threshold (using refined_scaled_mip)\n",
      "100_Sep12_18.37.00_181_1_constrained: 4 of 23 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 85 of 351 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.48.36_263_1_constrained: 129 of 406 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 112 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 21 of 212 particles above threshold (using refined_scaled_mip)\n",
      "138_Sep13_11.05.15_233_1_constrained: 214 of 661 particles above threshold (using refined_scaled_mip)\n",
      "144_Sep13_11.20.47_245_1_constrained: 126 of 311 particles above threshold (using refined_scaled_mip)\n",
      "94_Sep12_18.09.52_203_1_constrained: 128 of 314 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.14.14_205_1_constrained: 78 of 236 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 48 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 95 of 619 particles above threshold (using refined_scaled_mip)\n",
      "150_Sep13_11.35.16_257_1_constrained: 41 of 112 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 49 of 321 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.17.26_207_1_constrained: 125 of 405 particles above threshold (using refined_scaled_mip)\n",
      "77_Sep12_15.49.11_163_1_constrained: 135 of 197 particles above threshold (using refined_scaled_mip)\n",
      "105_Sep12_18.54.50_185_1_constrained: 74 of 189 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 46 of 398 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.30.28_255_1_constrained: 18 of 53 particles above threshold (using refined_scaled_mip)\n",
      "154_Sep13_11.39.01_265_1_constrained: 133 of 413 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_0-0_step1/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 79 of 516 particles above threshold (using refined_scaled_mip)\n",
      "98_Sep12_18.32.40_213_1_constrained: 70 of 299 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.46.34_261_1_constrained: 84 of 354 particles above threshold (using refined_scaled_mip)\n",
      "146_Sep13_10.20.11_249_1_constrained: 14 of 68 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_0-0_step1/82_Sep12_16.22.05_175_1_constrained_results.csv\n",
      "131_Sep13_10.52.39_221_1_constrained: 10 of 106 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.27.34_253_1_constrained: 106 of 257 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_0-0_step1/161_Sep13_13.14.49_275_1_constrained_results.csv\n",
      "  No particles above threshold in results_constrained_step1/psi_-13-2.5_theta_0-0_step1/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 8 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 48 of 316 particles above threshold (using refined_scaled_mip)\n",
      "  Saved 59 particles for 158_Sep13_13.03.40_267_1_constrained in step 1\n",
      "  Saved 91 particles for 140_Sep13_11.09.36_237_1_constrained in step 1\n",
      "  Saved 17 particles for 113_Sep12_19.20.31_197_1_constrained in step 1\n",
      "  Saved 109 particles for 151_Sep13_11.42.25_259_1_constrained in step 1\n",
      "  Saved 92 particles for 142_Sep13_11.01.22_241_1_constrained in step 1\n",
      "  Saved 129 particles for 79_Sep12_16.03.49_167_1_constrained in step 1\n",
      "  Saved 4 particles for 100_Sep12_18.37.00_181_1_constrained in step 1\n",
      "  Saved 85 particles for 163_Sep13_13.21.12_279_1_constrained in step 1\n",
      "  Saved 129 particles for 153_Sep13_11.48.36_263_1_constrained in step 1\n",
      "  Saved 112 particles for 141_Sep13_10.57.04_239_1_constrained in step 1\n",
      "  Saved 2 particles for 82_Sep12_16.17.55_173_1_constrained in step 1\n",
      "  Saved 21 particles for 128_Sep13_10.30.35_215_2_constrained in step 1\n",
      "  Saved 214 particles for 138_Sep13_11.05.15_233_1_constrained in step 1\n",
      "  Saved 126 particles for 144_Sep13_11.20.47_245_1_constrained in step 1\n",
      "  Saved 128 particles for 94_Sep12_18.09.52_203_1_constrained in step 1\n",
      "  Saved 78 particles for 95_Sep12_18.14.14_205_1_constrained in step 1\n",
      "  Saved 48 particles for 147_Sep13_11.25.08_251_1_constrained in step 1\n",
      "  Saved 95 particles for 139_Sep13_11.07.13_235_1_constrained in step 1\n",
      "  Saved 41 particles for 150_Sep13_11.35.16_257_1_constrained in step 1\n",
      "  Saved 49 particles for 143_Sep13_11.17.52_243_1_constrained in step 1\n",
      "  Saved 125 particles for 95_Sep12_18.17.26_207_1_constrained in step 1\n",
      "  Saved 135 particles for 77_Sep12_15.49.11_163_1_constrained in step 1\n",
      "  Saved 74 particles for 105_Sep12_18.54.50_185_1_constrained in step 1\n",
      "  Saved 46 particles for 97_Sep12_18.28.38_211_1_constrained in step 1\n",
      "  Saved 18 particles for 147_Sep13_11.30.28_255_1_constrained in step 1\n",
      "  Saved 133 particles for 154_Sep13_11.39.01_265_1_constrained in step 1\n",
      "  Saved 79 particles for 111_Sep12_19.09.17_193_1_constrained in step 1\n",
      "  Saved 70 particles for 98_Sep12_18.32.40_213_1_constrained in step 1\n",
      "  Saved 84 particles for 153_Sep13_11.46.34_261_1_constrained in step 1\n",
      "  Saved 14 particles for 146_Sep13_10.20.11_249_1_constrained in step 1\n",
      "  Saved 10 particles for 131_Sep13_10.52.39_221_1_constrained in step 1\n",
      "  Saved 106 particles for 147_Sep13_11.27.34_253_1_constrained in step 1\n",
      "  Saved 8 particles for 135_Sep13_10.23.29_229_1_constrained in step 1\n",
      "  Saved 48 particles for 93_Sep12_18.05.02_201_1_constrained in step 1\n",
      "  Saved 1 particles for 82_Sep12_16.22.05_175_1_constrained in step 1\n",
      "\n",
      "Processing Step 2: results_constrained_step3\n",
      "  Found 100 results files\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 891 correlations (total: 75573)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 891 correlations (total: 75573)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 891 correlations (total: 75573)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 891 correlations (total: 75573)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 891 correlations (total: 75573)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 891 correlations (total: 75573)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 891 correlations (total: 75573)\n",
      "  100_Sep12_18.37.00_181_1_constrained: Added 891 correlations (total: 3483)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 891 correlations (total: 75573)\n",
      "  153_Sep13_11.48.36_263_1_constrained: Added 891 correlations (total: 3483)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 891 correlations (total: 75573)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 891 correlations (total: 75573)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 891 correlations (total: 75573)\n",
      "  138_Sep13_11.05.15_233_1_constrained: Added 891 correlations (total: 3483)\n",
      "  144_Sep13_11.20.47_245_1_constrained: Added 891 correlations (total: 3483)\n",
      "  94_Sep12_18.09.52_203_1_constrained: Added 891 correlations (total: 3483)\n",
      "  95_Sep12_18.14.14_205_1_constrained: Added 891 correlations (total: 3483)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 891 correlations (total: 75573)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 891 correlations (total: 75573)\n",
      "  150_Sep13_11.35.16_257_1_constrained: Added 891 correlations (total: 3483)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 891 correlations (total: 75573)\n",
      "  95_Sep12_18.17.26_207_1_constrained: Added 891 correlations (total: 3483)\n",
      "  77_Sep12_15.49.11_163_1_constrained: Added 891 correlations (total: 3483)\n",
      "  105_Sep12_18.54.50_185_1_constrained: Added 891 correlations (total: 3483)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 891 correlations (total: 75573)\n",
      "  147_Sep13_11.30.28_255_1_constrained: Added 891 correlations (total: 3483)\n",
      "  154_Sep13_11.39.01_265_1_constrained: Added 891 correlations (total: 3483)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 891 correlations (total: 75573)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 891 correlations (total: 75573)\n",
      "  98_Sep12_18.32.40_213_1_constrained: Added 891 correlations (total: 3483)\n",
      "  153_Sep13_11.46.34_261_1_constrained: Added 891 correlations (total: 3483)\n",
      "  146_Sep13_10.20.11_249_1_constrained: Added 891 correlations (total: 3483)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 891 correlations (total: 75573)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 891 correlations (total: 75573)\n",
      "  147_Sep13_11.27.34_253_1_constrained: Added 891 correlations (total: 3483)\n",
      "  161_Sep13_13.14.49_275_1_constrained: Added 891 correlations (total: 3483)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 891 correlations (total: 75573)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 891 correlations (total: 75573)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 891 correlations (total: 75573)\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 1701 correlations (total: 77274)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 1701 correlations (total: 77274)\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 891 correlations (total: 78165)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 891 correlations (total: 78165)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 891 correlations (total: 78165)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 891 correlations (total: 78165)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 891 correlations (total: 78165)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 891 correlations (total: 78165)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 891 correlations (total: 78165)\n",
      "  100_Sep12_18.37.00_181_1_constrained: Added 891 correlations (total: 4374)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 891 correlations (total: 78165)\n",
      "  153_Sep13_11.48.36_263_1_constrained: Added 891 correlations (total: 4374)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 891 correlations (total: 78165)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 891 correlations (total: 78165)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 891 correlations (total: 78165)\n",
      "  138_Sep13_11.05.15_233_1_constrained: Added 891 correlations (total: 4374)\n",
      "  144_Sep13_11.20.47_245_1_constrained: Added 891 correlations (total: 4374)\n",
      "  94_Sep12_18.09.52_203_1_constrained: Added 891 correlations (total: 4374)\n",
      "  95_Sep12_18.14.14_205_1_constrained: Added 891 correlations (total: 4374)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 891 correlations (total: 78165)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 891 correlations (total: 78165)\n",
      "  150_Sep13_11.35.16_257_1_constrained: Added 891 correlations (total: 4374)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 891 correlations (total: 78165)\n",
      "  95_Sep12_18.17.26_207_1_constrained: Added 891 correlations (total: 4374)\n",
      "  77_Sep12_15.49.11_163_1_constrained: Added 891 correlations (total: 4374)\n",
      "  105_Sep12_18.54.50_185_1_constrained: Added 891 correlations (total: 4374)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 891 correlations (total: 78165)\n",
      "  147_Sep13_11.30.28_255_1_constrained: Added 891 correlations (total: 4374)\n",
      "  154_Sep13_11.39.01_265_1_constrained: Added 891 correlations (total: 4374)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 891 correlations (total: 78165)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 891 correlations (total: 78165)\n",
      "  98_Sep12_18.32.40_213_1_constrained: Added 891 correlations (total: 4374)\n",
      "  153_Sep13_11.46.34_261_1_constrained: Added 891 correlations (total: 4374)\n",
      "  146_Sep13_10.20.11_249_1_constrained: Added 891 correlations (total: 4374)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 891 correlations (total: 78165)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 891 correlations (total: 78165)\n",
      "  147_Sep13_11.27.34_253_1_constrained: Added 891 correlations (total: 4374)\n",
      "  161_Sep13_13.14.49_275_1_constrained: Added 891 correlations (total: 4374)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 891 correlations (total: 78165)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 891 correlations (total: 78165)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 891 correlations (total: 78165)\n",
      "  Threshold for 158_Sep13_13.03.40_267_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 140_Sep13_11.09.36_237_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 113_Sep12_19.20.31_197_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 151_Sep13_11.42.25_259_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 107_Sep12_18.57.58_187_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 142_Sep13_11.01.22_241_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 79_Sep12_16.03.49_167_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 100_Sep12_18.37.00_181_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 163_Sep13_13.21.12_279_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 153_Sep13_11.48.36_263_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 141_Sep13_10.57.04_239_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 82_Sep12_16.17.55_173_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 128_Sep13_10.30.35_215_2_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 138_Sep13_11.05.15_233_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 144_Sep13_11.20.47_245_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 94_Sep12_18.09.52_203_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 95_Sep12_18.14.14_205_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 147_Sep13_11.25.08_251_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 139_Sep13_11.07.13_235_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 150_Sep13_11.35.16_257_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 143_Sep13_11.17.52_243_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 95_Sep12_18.17.26_207_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 77_Sep12_15.49.11_163_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 105_Sep12_18.54.50_185_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 97_Sep12_18.28.38_211_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 147_Sep13_11.30.28_255_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 154_Sep13_11.39.01_265_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 25_Sep12_11.40.05_145_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 111_Sep12_19.09.17_193_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 98_Sep12_18.32.40_213_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 153_Sep13_11.46.34_261_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 146_Sep13_10.20.11_249_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 82_Sep12_16.22.05_175_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 131_Sep13_10.52.39_221_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 147_Sep13_11.27.34_253_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 161_Sep13_13.14.49_275_1_constrained in step 2: 4.7263 (based on 4374 total correlations)\n",
      "  Threshold for 44_Sep12_12.54.40_150_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 135_Sep13_10.23.29_229_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "  Threshold for 93_Sep12_18.05.02_201_1_constrained in step 2: 5.2818 (based on 78165 total correlations)\n",
      "158_Sep13_13.03.40_267_1_constrained: 119 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 168 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 31 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 171 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step3/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 161 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 146 of 236 particles above threshold (using refined_scaled_mip)\n",
      "100_Sep12_18.37.00_181_1_constrained: 8 of 23 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 157 of 351 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.48.36_263_1_constrained: 182 of 406 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 199 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 37 of 212 particles above threshold (using refined_scaled_mip)\n",
      "138_Sep13_11.05.15_233_1_constrained: 319 of 661 particles above threshold (using refined_scaled_mip)\n",
      "144_Sep13_11.20.47_245_1_constrained: 172 of 311 particles above threshold (using refined_scaled_mip)\n",
      "94_Sep12_18.09.52_203_1_constrained: 174 of 314 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.14.14_205_1_constrained: 111 of 236 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 102 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 214 of 619 particles above threshold (using refined_scaled_mip)\n",
      "150_Sep13_11.35.16_257_1_constrained: 58 of 112 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 93 of 321 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.17.26_207_1_constrained: 175 of 405 particles above threshold (using refined_scaled_mip)\n",
      "77_Sep12_15.49.11_163_1_constrained: 141 of 197 particles above threshold (using refined_scaled_mip)\n",
      "105_Sep12_18.54.50_185_1_constrained: 101 of 189 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 95 of 398 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.30.28_255_1_constrained: 22 of 53 particles above threshold (using refined_scaled_mip)\n",
      "154_Sep13_11.39.01_265_1_constrained: 207 of 413 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step3/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 132 of 516 particles above threshold (using refined_scaled_mip)\n",
      "98_Sep12_18.32.40_213_1_constrained: 96 of 299 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.46.34_261_1_constrained: 137 of 354 particles above threshold (using refined_scaled_mip)\n",
      "146_Sep13_10.20.11_249_1_constrained: 31 of 68 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.22.05_175_1_constrained: 1 of 29 particles above threshold (using refined_scaled_mip)\n",
      "131_Sep13_10.52.39_221_1_constrained: 24 of 106 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.27.34_253_1_constrained: 147 of 257 particles above threshold (using refined_scaled_mip)\n",
      "161_Sep13_13.14.49_275_1_constrained: 1 of 13 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step3/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 12 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 101 of 316 particles above threshold (using refined_scaled_mip)\n",
      "158_Sep13_13.03.40_267_1_constrained: 123 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 165 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 33 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 177 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step3/theta_-8-2_step0.5/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 171 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 147 of 236 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 168 of 351 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 207 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 40 of 212 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 101 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 227 of 619 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 96 of 321 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 104 of 398 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step3/theta_-8-2_step0.5/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 141 of 516 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.22.05_175_1_constrained: 1 of 29 particles above threshold (using refined_scaled_mip)\n",
      "131_Sep13_10.52.39_221_1_constrained: 22 of 106 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step3/theta_-8-2_step0.5/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 13 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 109 of 316 particles above threshold (using refined_scaled_mip)\n",
      "158_Sep13_13.03.40_267_1_constrained: 117 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 163 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 31 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 173 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step3/theta_-8-2_step1/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 163 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 144 of 236 particles above threshold (using refined_scaled_mip)\n",
      "100_Sep12_18.37.00_181_1_constrained: 8 of 23 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 164 of 351 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.48.36_263_1_constrained: 172 of 406 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 198 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 36 of 212 particles above threshold (using refined_scaled_mip)\n",
      "138_Sep13_11.05.15_233_1_constrained: 320 of 661 particles above threshold (using refined_scaled_mip)\n",
      "144_Sep13_11.20.47_245_1_constrained: 173 of 311 particles above threshold (using refined_scaled_mip)\n",
      "94_Sep12_18.09.52_203_1_constrained: 174 of 314 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.14.14_205_1_constrained: 112 of 236 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 94 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 218 of 619 particles above threshold (using refined_scaled_mip)\n",
      "150_Sep13_11.35.16_257_1_constrained: 52 of 112 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 92 of 321 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.17.26_207_1_constrained: 176 of 405 particles above threshold (using refined_scaled_mip)\n",
      "77_Sep12_15.49.11_163_1_constrained: 142 of 197 particles above threshold (using refined_scaled_mip)\n",
      "105_Sep12_18.54.50_185_1_constrained: 99 of 189 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 100 of 398 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.30.28_255_1_constrained: 24 of 53 particles above threshold (using refined_scaled_mip)\n",
      "154_Sep13_11.39.01_265_1_constrained: 207 of 413 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step3/theta_-8-2_step1/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 131 of 516 particles above threshold (using refined_scaled_mip)\n",
      "98_Sep12_18.32.40_213_1_constrained: 96 of 299 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.46.34_261_1_constrained: 125 of 354 particles above threshold (using refined_scaled_mip)\n",
      "146_Sep13_10.20.11_249_1_constrained: 31 of 68 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.22.05_175_1_constrained: 1 of 29 particles above threshold (using refined_scaled_mip)\n",
      "131_Sep13_10.52.39_221_1_constrained: 22 of 106 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.27.34_253_1_constrained: 145 of 257 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step3/theta_-8-2_step1/161_Sep13_13.14.49_275_1_constrained_results.csv\n",
      "  No particles above threshold in results_constrained_step3/theta_-8-2_step1/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 13 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 101 of 316 particles above threshold (using refined_scaled_mip)\n",
      "  Saved 127 particles for 158_Sep13_13.03.40_267_1_constrained in step 2\n",
      "  Saved 172 particles for 140_Sep13_11.09.36_237_1_constrained in step 2\n",
      "  Saved 34 particles for 113_Sep12_19.20.31_197_1_constrained in step 2\n",
      "  Saved 180 particles for 151_Sep13_11.42.25_259_1_constrained in step 2\n",
      "  Saved 174 particles for 142_Sep13_11.01.22_241_1_constrained in step 2\n",
      "  Saved 149 particles for 79_Sep12_16.03.49_167_1_constrained in step 2\n",
      "  Saved 9 particles for 100_Sep12_18.37.00_181_1_constrained in step 2\n",
      "  Saved 168 particles for 163_Sep13_13.21.12_279_1_constrained in step 2\n",
      "  Saved 186 particles for 153_Sep13_11.48.36_263_1_constrained in step 2\n",
      "  Saved 211 particles for 141_Sep13_10.57.04_239_1_constrained in step 2\n",
      "  Saved 2 particles for 82_Sep12_16.17.55_173_1_constrained in step 2\n",
      "  Saved 41 particles for 128_Sep13_10.30.35_215_2_constrained in step 2\n",
      "  Saved 334 particles for 138_Sep13_11.05.15_233_1_constrained in step 2\n",
      "  Saved 178 particles for 144_Sep13_11.20.47_245_1_constrained in step 2\n",
      "  Saved 178 particles for 94_Sep12_18.09.52_203_1_constrained in step 2\n",
      "  Saved 116 particles for 95_Sep12_18.14.14_205_1_constrained in step 2\n",
      "  Saved 111 particles for 147_Sep13_11.25.08_251_1_constrained in step 2\n",
      "  Saved 236 particles for 139_Sep13_11.07.13_235_1_constrained in step 2\n",
      "  Saved 58 particles for 150_Sep13_11.35.16_257_1_constrained in step 2\n",
      "  Saved 98 particles for 143_Sep13_11.17.52_243_1_constrained in step 2\n",
      "  Saved 183 particles for 95_Sep12_18.17.26_207_1_constrained in step 2\n",
      "  Saved 144 particles for 77_Sep12_15.49.11_163_1_constrained in step 2\n",
      "  Saved 103 particles for 105_Sep12_18.54.50_185_1_constrained in step 2\n",
      "  Saved 106 particles for 97_Sep12_18.28.38_211_1_constrained in step 2\n",
      "  Saved 25 particles for 147_Sep13_11.30.28_255_1_constrained in step 2\n",
      "  Saved 213 particles for 154_Sep13_11.39.01_265_1_constrained in step 2\n",
      "  Saved 145 particles for 111_Sep12_19.09.17_193_1_constrained in step 2\n",
      "  Saved 100 particles for 98_Sep12_18.32.40_213_1_constrained in step 2\n",
      "  Saved 138 particles for 153_Sep13_11.46.34_261_1_constrained in step 2\n",
      "  Saved 32 particles for 146_Sep13_10.20.11_249_1_constrained in step 2\n",
      "  Saved 25 particles for 131_Sep13_10.52.39_221_1_constrained in step 2\n",
      "  Saved 150 particles for 147_Sep13_11.27.34_253_1_constrained in step 2\n",
      "  Saved 13 particles for 135_Sep13_10.23.29_229_1_constrained in step 2\n",
      "  Saved 111 particles for 93_Sep12_18.05.02_201_1_constrained in step 2\n",
      "  Saved 1 particles for 82_Sep12_16.22.05_175_1_constrained in step 2\n",
      "  Saved 1 particles for 161_Sep13_13.14.49_275_1_constrained in step 2\n",
      "\n",
      "Processing Step 3: results_constrained_step4\n",
      "  Found 100 results files\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  100_Sep12_18.37.00_181_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  153_Sep13_11.48.36_263_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 34020 correlations (total: 112185)\n",
      "  138_Sep13_11.05.15_233_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  144_Sep13_11.20.47_245_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  94_Sep12_18.09.52_203_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  95_Sep12_18.14.14_205_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  150_Sep13_11.35.16_257_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  95_Sep12_18.17.26_207_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  77_Sep12_15.49.11_163_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  105_Sep12_18.54.50_185_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  147_Sep13_11.30.28_255_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  154_Sep13_11.39.01_265_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  98_Sep12_18.32.40_213_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  153_Sep13_11.46.34_261_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  146_Sep13_10.20.11_249_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  147_Sep13_11.27.34_253_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  161_Sep13_13.14.49_275_1_constrained: Added 34020 correlations (total: 38394)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 34020 correlations (total: 112185)\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 34020 correlations (total: 146205)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 34020 correlations (total: 146205)\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  100_Sep12_18.37.00_181_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  153_Sep13_11.48.36_263_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 34020 correlations (total: 180225)\n",
      "  138_Sep13_11.05.15_233_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  144_Sep13_11.20.47_245_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  94_Sep12_18.09.52_203_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  95_Sep12_18.14.14_205_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  150_Sep13_11.35.16_257_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  95_Sep12_18.17.26_207_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  77_Sep12_15.49.11_163_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  105_Sep12_18.54.50_185_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  147_Sep13_11.30.28_255_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  154_Sep13_11.39.01_265_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  98_Sep12_18.32.40_213_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  153_Sep13_11.46.34_261_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  146_Sep13_10.20.11_249_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  147_Sep13_11.27.34_253_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  161_Sep13_13.14.49_275_1_constrained: Added 34020 correlations (total: 72414)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 34020 correlations (total: 180225)\n",
      "  Threshold for 158_Sep13_13.03.40_267_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 140_Sep13_11.09.36_237_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 113_Sep12_19.20.31_197_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 151_Sep13_11.42.25_259_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 107_Sep12_18.57.58_187_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 142_Sep13_11.01.22_241_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 79_Sep12_16.03.49_167_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 100_Sep12_18.37.00_181_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 163_Sep13_13.21.12_279_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 153_Sep13_11.48.36_263_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 141_Sep13_10.57.04_239_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 82_Sep12_16.17.55_173_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 128_Sep13_10.30.35_215_2_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 138_Sep13_11.05.15_233_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 144_Sep13_11.20.47_245_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 94_Sep12_18.09.52_203_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 95_Sep12_18.14.14_205_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 147_Sep13_11.25.08_251_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 139_Sep13_11.07.13_235_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 150_Sep13_11.35.16_257_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 143_Sep13_11.17.52_243_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 95_Sep12_18.17.26_207_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 77_Sep12_15.49.11_163_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 105_Sep12_18.54.50_185_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 97_Sep12_18.28.38_211_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 147_Sep13_11.30.28_255_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 154_Sep13_11.39.01_265_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 25_Sep12_11.40.05_145_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 111_Sep12_19.09.17_193_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 98_Sep12_18.32.40_213_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 153_Sep13_11.46.34_261_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 146_Sep13_10.20.11_249_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 82_Sep12_16.22.05_175_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 131_Sep13_10.52.39_221_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 147_Sep13_11.27.34_253_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 161_Sep13_13.14.49_275_1_constrained in step 3: 5.2678 (based on 72414 total correlations)\n",
      "  Threshold for 44_Sep12_12.54.40_150_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 135_Sep13_10.23.29_229_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "  Threshold for 93_Sep12_18.05.02_201_1_constrained in step 3: 5.4328 (based on 180225 total correlations)\n",
      "158_Sep13_13.03.40_267_1_constrained: 153 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 219 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 43 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 233 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step4/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 216 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 146 of 236 particles above threshold (using refined_scaled_mip)\n",
      "100_Sep12_18.37.00_181_1_constrained: 7 of 23 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 200 of 351 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.48.36_263_1_constrained: 161 of 406 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 257 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 52 of 212 particles above threshold (using refined_scaled_mip)\n",
      "138_Sep13_11.05.15_233_1_constrained: 292 of 661 particles above threshold (using refined_scaled_mip)\n",
      "144_Sep13_11.20.47_245_1_constrained: 177 of 311 particles above threshold (using refined_scaled_mip)\n",
      "94_Sep12_18.09.52_203_1_constrained: 166 of 314 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.14.14_205_1_constrained: 105 of 236 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 130 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 280 of 619 particles above threshold (using refined_scaled_mip)\n",
      "150_Sep13_11.35.16_257_1_constrained: 49 of 112 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 127 of 321 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.17.26_207_1_constrained: 161 of 405 particles above threshold (using refined_scaled_mip)\n",
      "77_Sep12_15.49.11_163_1_constrained: 136 of 197 particles above threshold (using refined_scaled_mip)\n",
      "105_Sep12_18.54.50_185_1_constrained: 87 of 189 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 118 of 398 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.30.28_255_1_constrained: 18 of 53 particles above threshold (using refined_scaled_mip)\n",
      "154_Sep13_11.39.01_265_1_constrained: 203 of 413 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step4/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 174 of 516 particles above threshold (using refined_scaled_mip)\n",
      "98_Sep12_18.32.40_213_1_constrained: 88 of 299 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.46.34_261_1_constrained: 119 of 354 particles above threshold (using refined_scaled_mip)\n",
      "146_Sep13_10.20.11_249_1_constrained: 25 of 68 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.22.05_175_1_constrained: 1 of 29 particles above threshold (using refined_scaled_mip)\n",
      "131_Sep13_10.52.39_221_1_constrained: 31 of 106 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.27.34_253_1_constrained: 142 of 257 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step4/161_Sep13_13.14.49_275_1_constrained_results.csv\n",
      "  No particles above threshold in results_constrained_step4/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 15 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 125 of 316 particles above threshold (using refined_scaled_mip)\n",
      "158_Sep13_13.03.40_267_1_constrained: 124 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 179 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 32 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 196 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step4/refine_0.1/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 181 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 142 of 236 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 177 of 351 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 218 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 37 of 212 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 106 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 235 of 619 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 105 of 321 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 101 of 398 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step4/refine_0.1/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 143 of 516 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.22.05_175_1_constrained: 1 of 29 particles above threshold (using refined_scaled_mip)\n",
      "131_Sep13_10.52.39_221_1_constrained: 23 of 106 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step4/refine_0.1/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 13 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 102 of 316 particles above threshold (using refined_scaled_mip)\n",
      "158_Sep13_13.03.40_267_1_constrained: 154 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 217 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 40 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 230 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step4/psi_-5-5_theta_-5-5_step0.5/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 211 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 145 of 236 particles above threshold (using refined_scaled_mip)\n",
      "100_Sep12_18.37.00_181_1_constrained: 7 of 23 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 199 of 351 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.48.36_263_1_constrained: 162 of 406 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 254 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 50 of 212 particles above threshold (using refined_scaled_mip)\n",
      "138_Sep13_11.05.15_233_1_constrained: 298 of 661 particles above threshold (using refined_scaled_mip)\n",
      "144_Sep13_11.20.47_245_1_constrained: 179 of 311 particles above threshold (using refined_scaled_mip)\n",
      "94_Sep12_18.09.52_203_1_constrained: 164 of 314 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.14.14_205_1_constrained: 105 of 236 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 125 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 283 of 619 particles above threshold (using refined_scaled_mip)\n",
      "150_Sep13_11.35.16_257_1_constrained: 48 of 112 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 124 of 321 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.17.26_207_1_constrained: 162 of 405 particles above threshold (using refined_scaled_mip)\n",
      "77_Sep12_15.49.11_163_1_constrained: 139 of 197 particles above threshold (using refined_scaled_mip)\n",
      "105_Sep12_18.54.50_185_1_constrained: 91 of 189 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 119 of 398 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.30.28_255_1_constrained: 17 of 53 particles above threshold (using refined_scaled_mip)\n",
      "154_Sep13_11.39.01_265_1_constrained: 200 of 413 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step4/psi_-5-5_theta_-5-5_step0.5/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 177 of 516 particles above threshold (using refined_scaled_mip)\n",
      "98_Sep12_18.32.40_213_1_constrained: 89 of 299 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.46.34_261_1_constrained: 107 of 354 particles above threshold (using refined_scaled_mip)\n",
      "146_Sep13_10.20.11_249_1_constrained: 24 of 68 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.22.05_175_1_constrained: 1 of 29 particles above threshold (using refined_scaled_mip)\n",
      "131_Sep13_10.52.39_221_1_constrained: 32 of 106 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.27.34_253_1_constrained: 138 of 257 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step4/psi_-5-5_theta_-5-5_step0.5/161_Sep13_13.14.49_275_1_constrained_results.csv\n",
      "  No particles above threshold in results_constrained_step4/psi_-5-5_theta_-5-5_step0.5/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 15 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 127 of 316 particles above threshold (using refined_scaled_mip)\n",
      "  Saved 158 particles for 158_Sep13_13.03.40_267_1_constrained in step 3\n",
      "  Saved 228 particles for 140_Sep13_11.09.36_237_1_constrained in step 3\n",
      "  Saved 45 particles for 113_Sep12_19.20.31_197_1_constrained in step 3\n",
      "  Saved 237 particles for 151_Sep13_11.42.25_259_1_constrained in step 3\n",
      "  Saved 221 particles for 142_Sep13_11.01.22_241_1_constrained in step 3\n",
      "  Saved 149 particles for 79_Sep12_16.03.49_167_1_constrained in step 3\n",
      "  Saved 7 particles for 100_Sep12_18.37.00_181_1_constrained in step 3\n",
      "  Saved 205 particles for 163_Sep13_13.21.12_279_1_constrained in step 3\n",
      "  Saved 170 particles for 153_Sep13_11.48.36_263_1_constrained in step 3\n",
      "  Saved 271 particles for 141_Sep13_10.57.04_239_1_constrained in step 3\n",
      "  Saved 2 particles for 82_Sep12_16.17.55_173_1_constrained in step 3\n",
      "  Saved 56 particles for 128_Sep13_10.30.35_215_2_constrained in step 3\n",
      "  Saved 308 particles for 138_Sep13_11.05.15_233_1_constrained in step 3\n",
      "  Saved 184 particles for 144_Sep13_11.20.47_245_1_constrained in step 3\n",
      "  Saved 173 particles for 94_Sep12_18.09.52_203_1_constrained in step 3\n",
      "  Saved 111 particles for 95_Sep12_18.14.14_205_1_constrained in step 3\n",
      "  Saved 144 particles for 147_Sep13_11.25.08_251_1_constrained in step 3\n",
      "  Saved 298 particles for 139_Sep13_11.07.13_235_1_constrained in step 3\n",
      "  Saved 51 particles for 150_Sep13_11.35.16_257_1_constrained in step 3\n",
      "  Saved 131 particles for 143_Sep13_11.17.52_243_1_constrained in step 3\n",
      "  Saved 169 particles for 95_Sep12_18.17.26_207_1_constrained in step 3\n",
      "  Saved 139 particles for 77_Sep12_15.49.11_163_1_constrained in step 3\n",
      "  Saved 91 particles for 105_Sep12_18.54.50_185_1_constrained in step 3\n",
      "  Saved 126 particles for 97_Sep12_18.28.38_211_1_constrained in step 3\n",
      "  Saved 18 particles for 147_Sep13_11.30.28_255_1_constrained in step 3\n",
      "  Saved 210 particles for 154_Sep13_11.39.01_265_1_constrained in step 3\n",
      "  Saved 185 particles for 111_Sep12_19.09.17_193_1_constrained in step 3\n",
      "  Saved 94 particles for 98_Sep12_18.32.40_213_1_constrained in step 3\n",
      "  Saved 123 particles for 153_Sep13_11.46.34_261_1_constrained in step 3\n",
      "  Saved 25 particles for 146_Sep13_10.20.11_249_1_constrained in step 3\n",
      "  Saved 32 particles for 131_Sep13_10.52.39_221_1_constrained in step 3\n",
      "  Saved 147 particles for 147_Sep13_11.27.34_253_1_constrained in step 3\n",
      "  Saved 16 particles for 135_Sep13_10.23.29_229_1_constrained in step 3\n",
      "  Saved 132 particles for 93_Sep12_18.05.02_201_1_constrained in step 3\n",
      "  Saved 1 particles for 82_Sep12_16.22.05_175_1_constrained in step 3\n",
      "\n",
      "Processing Step 4: results_constrained_step5\n",
      "  Found 78 results files\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  100_Sep12_18.37.00_181_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  153_Sep13_11.48.36_263_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 98010 correlations (total: 278235)\n",
      "  138_Sep13_11.05.15_233_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  144_Sep13_11.20.47_245_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  94_Sep12_18.09.52_203_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  95_Sep12_18.14.14_205_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  150_Sep13_11.35.16_257_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  95_Sep12_18.17.26_207_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  77_Sep12_15.49.11_163_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  105_Sep12_18.54.50_185_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  147_Sep13_11.30.28_255_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  154_Sep13_11.39.01_265_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  98_Sep12_18.32.40_213_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  153_Sep13_11.46.34_261_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  146_Sep13_10.20.11_249_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  147_Sep13_11.27.34_253_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  161_Sep13_13.14.49_275_1_constrained: Added 98010 correlations (total: 170424)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 98010 correlations (total: 278235)\n",
      "  158_Sep13_13.03.40_267_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  140_Sep13_11.09.36_237_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  113_Sep12_19.20.31_197_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  151_Sep13_11.42.25_259_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  107_Sep12_18.57.58_187_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  142_Sep13_11.01.22_241_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  79_Sep12_16.03.49_167_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  100_Sep12_18.37.00_181_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  163_Sep13_13.21.12_279_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  153_Sep13_11.48.36_263_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  141_Sep13_10.57.04_239_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  82_Sep12_16.17.55_173_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  128_Sep13_10.30.35_215_2_constrained: Added 98010 correlations (total: 376245)\n",
      "  138_Sep13_11.05.15_233_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  144_Sep13_11.20.47_245_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  94_Sep12_18.09.52_203_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  95_Sep12_18.14.14_205_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  147_Sep13_11.25.08_251_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  139_Sep13_11.07.13_235_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  150_Sep13_11.35.16_257_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  143_Sep13_11.17.52_243_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  95_Sep12_18.17.26_207_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  77_Sep12_15.49.11_163_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  105_Sep12_18.54.50_185_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  97_Sep12_18.28.38_211_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  147_Sep13_11.30.28_255_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  154_Sep13_11.39.01_265_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  25_Sep12_11.40.05_145_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  111_Sep12_19.09.17_193_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  98_Sep12_18.32.40_213_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  153_Sep13_11.46.34_261_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  146_Sep13_10.20.11_249_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  82_Sep12_16.22.05_175_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  131_Sep13_10.52.39_221_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  147_Sep13_11.27.34_253_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  161_Sep13_13.14.49_275_1_constrained: Added 98010 correlations (total: 268434)\n",
      "  44_Sep12_12.54.40_150_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  135_Sep13_10.23.29_229_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  93_Sep12_18.05.02_201_1_constrained: Added 98010 correlations (total: 376245)\n",
      "  Threshold for 158_Sep13_13.03.40_267_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 140_Sep13_11.09.36_237_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 113_Sep12_19.20.31_197_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 151_Sep13_11.42.25_259_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 107_Sep12_18.57.58_187_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 142_Sep13_11.01.22_241_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 79_Sep12_16.03.49_167_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 100_Sep12_18.37.00_181_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 163_Sep13_13.21.12_279_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 153_Sep13_11.48.36_263_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 141_Sep13_10.57.04_239_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 82_Sep12_16.17.55_173_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 128_Sep13_10.30.35_215_2_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 138_Sep13_11.05.15_233_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 144_Sep13_11.20.47_245_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 94_Sep12_18.09.52_203_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 95_Sep12_18.14.14_205_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 147_Sep13_11.25.08_251_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 139_Sep13_11.07.13_235_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 150_Sep13_11.35.16_257_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 143_Sep13_11.17.52_243_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 95_Sep12_18.17.26_207_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 77_Sep12_15.49.11_163_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 105_Sep12_18.54.50_185_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 97_Sep12_18.28.38_211_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 147_Sep13_11.30.28_255_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 154_Sep13_11.39.01_265_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 25_Sep12_11.40.05_145_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 111_Sep12_19.09.17_193_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 98_Sep12_18.32.40_213_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 153_Sep13_11.46.34_261_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 146_Sep13_10.20.11_249_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 82_Sep12_16.22.05_175_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 131_Sep13_10.52.39_221_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 147_Sep13_11.27.34_253_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 161_Sep13_13.14.49_275_1_constrained in step 4: 5.5034 (based on 268434 total correlations)\n",
      "  Threshold for 44_Sep12_12.54.40_150_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 135_Sep13_10.23.29_229_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "  Threshold for 93_Sep12_18.05.02_201_1_constrained in step 4: 5.5626 (based on 376245 total correlations)\n",
      "158_Sep13_13.03.40_267_1_constrained: 152 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 220 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 47 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 233 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step5/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 210 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 145 of 236 particles above threshold (using refined_scaled_mip)\n",
      "100_Sep12_18.37.00_181_1_constrained: 6 of 23 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 201 of 351 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.48.36_263_1_constrained: 146 of 406 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 256 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 49 of 212 particles above threshold (using refined_scaled_mip)\n",
      "138_Sep13_11.05.15_233_1_constrained: 270 of 661 particles above threshold (using refined_scaled_mip)\n",
      "144_Sep13_11.20.47_245_1_constrained: 170 of 311 particles above threshold (using refined_scaled_mip)\n",
      "94_Sep12_18.09.52_203_1_constrained: 157 of 314 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.14.14_205_1_constrained: 92 of 236 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 128 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 276 of 619 particles above threshold (using refined_scaled_mip)\n",
      "150_Sep13_11.35.16_257_1_constrained: 36 of 112 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 126 of 321 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.17.26_207_1_constrained: 155 of 405 particles above threshold (using refined_scaled_mip)\n",
      "77_Sep12_15.49.11_163_1_constrained: 132 of 197 particles above threshold (using refined_scaled_mip)\n",
      "105_Sep12_18.54.50_185_1_constrained: 83 of 189 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 118 of 398 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.30.28_255_1_constrained: 17 of 53 particles above threshold (using refined_scaled_mip)\n",
      "154_Sep13_11.39.01_265_1_constrained: 185 of 413 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step5/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 178 of 516 particles above threshold (using refined_scaled_mip)\n",
      "98_Sep12_18.32.40_213_1_constrained: 71 of 299 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.46.34_261_1_constrained: 105 of 354 particles above threshold (using refined_scaled_mip)\n",
      "146_Sep13_10.20.11_249_1_constrained: 24 of 68 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.22.05_175_1_constrained: 1 of 29 particles above threshold (using refined_scaled_mip)\n",
      "131_Sep13_10.52.39_221_1_constrained: 33 of 106 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.27.34_253_1_constrained: 135 of 257 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step5/161_Sep13_13.14.49_275_1_constrained_results.csv\n",
      "  No particles above threshold in results_constrained_step5/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 14 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 120 of 316 particles above threshold (using refined_scaled_mip)\n",
      "158_Sep13_13.03.40_267_1_constrained: 152 of 362 particles above threshold (using refined_scaled_mip)\n",
      "140_Sep13_11.09.36_237_1_constrained: 221 of 489 particles above threshold (using refined_scaled_mip)\n",
      "113_Sep12_19.20.31_197_1_constrained: 45 of 223 particles above threshold (using refined_scaled_mip)\n",
      "151_Sep13_11.42.25_259_1_constrained: 230 of 421 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step5/refine_0.1_defocus_search/107_Sep12_18.57.58_187_1_constrained_results.csv\n",
      "142_Sep13_11.01.22_241_1_constrained: 212 of 434 particles above threshold (using refined_scaled_mip)\n",
      "79_Sep12_16.03.49_167_1_constrained: 144 of 236 particles above threshold (using refined_scaled_mip)\n",
      "100_Sep12_18.37.00_181_1_constrained: 6 of 23 particles above threshold (using refined_scaled_mip)\n",
      "163_Sep13_13.21.12_279_1_constrained: 196 of 351 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.48.36_263_1_constrained: 145 of 406 particles above threshold (using refined_scaled_mip)\n",
      "141_Sep13_10.57.04_239_1_constrained: 254 of 678 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.17.55_173_1_constrained: 2 of 31 particles above threshold (using refined_scaled_mip)\n",
      "128_Sep13_10.30.35_215_2_constrained: 50 of 212 particles above threshold (using refined_scaled_mip)\n",
      "138_Sep13_11.05.15_233_1_constrained: 280 of 661 particles above threshold (using refined_scaled_mip)\n",
      "144_Sep13_11.20.47_245_1_constrained: 170 of 311 particles above threshold (using refined_scaled_mip)\n",
      "94_Sep12_18.09.52_203_1_constrained: 154 of 314 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.14.14_205_1_constrained: 95 of 236 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.25.08_251_1_constrained: 126 of 400 particles above threshold (using refined_scaled_mip)\n",
      "139_Sep13_11.07.13_235_1_constrained: 276 of 619 particles above threshold (using refined_scaled_mip)\n",
      "150_Sep13_11.35.16_257_1_constrained: 41 of 112 particles above threshold (using refined_scaled_mip)\n",
      "143_Sep13_11.17.52_243_1_constrained: 127 of 321 particles above threshold (using refined_scaled_mip)\n",
      "95_Sep12_18.17.26_207_1_constrained: 155 of 405 particles above threshold (using refined_scaled_mip)\n",
      "77_Sep12_15.49.11_163_1_constrained: 133 of 197 particles above threshold (using refined_scaled_mip)\n",
      "105_Sep12_18.54.50_185_1_constrained: 83 of 189 particles above threshold (using refined_scaled_mip)\n",
      "97_Sep12_18.28.38_211_1_constrained: 117 of 398 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.30.28_255_1_constrained: 17 of 53 particles above threshold (using refined_scaled_mip)\n",
      "154_Sep13_11.39.01_265_1_constrained: 186 of 413 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step5/refine_0.1_defocus_search/25_Sep12_11.40.05_145_1_constrained_results.csv\n",
      "111_Sep12_19.09.17_193_1_constrained: 176 of 516 particles above threshold (using refined_scaled_mip)\n",
      "98_Sep12_18.32.40_213_1_constrained: 74 of 299 particles above threshold (using refined_scaled_mip)\n",
      "153_Sep13_11.46.34_261_1_constrained: 93 of 354 particles above threshold (using refined_scaled_mip)\n",
      "146_Sep13_10.20.11_249_1_constrained: 23 of 68 particles above threshold (using refined_scaled_mip)\n",
      "82_Sep12_16.22.05_175_1_constrained: 1 of 29 particles above threshold (using refined_scaled_mip)\n",
      "131_Sep13_10.52.39_221_1_constrained: 32 of 106 particles above threshold (using refined_scaled_mip)\n",
      "147_Sep13_11.27.34_253_1_constrained: 136 of 257 particles above threshold (using refined_scaled_mip)\n",
      "  No particles above threshold in results_constrained_step5/refine_0.1_defocus_search/161_Sep13_13.14.49_275_1_constrained_results.csv\n",
      "  No particles above threshold in results_constrained_step5/refine_0.1_defocus_search/44_Sep12_12.54.40_150_1_constrained_results.csv\n",
      "135_Sep13_10.23.29_229_1_constrained: 14 of 46 particles above threshold (using refined_scaled_mip)\n",
      "93_Sep12_18.05.02_201_1_constrained: 124 of 316 particles above threshold (using refined_scaled_mip)\n",
      "  Saved 159 particles for 158_Sep13_13.03.40_267_1_constrained in step 4\n",
      "  Saved 227 particles for 140_Sep13_11.09.36_237_1_constrained in step 4\n",
      "  Saved 47 particles for 113_Sep12_19.20.31_197_1_constrained in step 4\n",
      "  Saved 235 particles for 151_Sep13_11.42.25_259_1_constrained in step 4\n",
      "  Saved 216 particles for 142_Sep13_11.01.22_241_1_constrained in step 4\n",
      "  Saved 147 particles for 79_Sep12_16.03.49_167_1_constrained in step 4\n",
      "  Saved 6 particles for 100_Sep12_18.37.00_181_1_constrained in step 4\n",
      "  Saved 202 particles for 163_Sep13_13.21.12_279_1_constrained in step 4\n",
      "  Saved 155 particles for 153_Sep13_11.48.36_263_1_constrained in step 4\n",
      "  Saved 264 particles for 141_Sep13_10.57.04_239_1_constrained in step 4\n",
      "  Saved 2 particles for 82_Sep12_16.17.55_173_1_constrained in step 4\n",
      "  Saved 53 particles for 128_Sep13_10.30.35_215_2_constrained in step 4\n",
      "  Saved 289 particles for 138_Sep13_11.05.15_233_1_constrained in step 4\n",
      "  Saved 176 particles for 144_Sep13_11.20.47_245_1_constrained in step 4\n",
      "  Saved 163 particles for 94_Sep12_18.09.52_203_1_constrained in step 4\n",
      "  Saved 97 particles for 95_Sep12_18.14.14_205_1_constrained in step 4\n",
      "  Saved 138 particles for 147_Sep13_11.25.08_251_1_constrained in step 4\n",
      "  Saved 286 particles for 139_Sep13_11.07.13_235_1_constrained in step 4\n",
      "  Saved 42 particles for 150_Sep13_11.35.16_257_1_constrained in step 4\n",
      "  Saved 130 particles for 143_Sep13_11.17.52_243_1_constrained in step 4\n",
      "  Saved 160 particles for 95_Sep12_18.17.26_207_1_constrained in step 4\n",
      "  Saved 135 particles for 77_Sep12_15.49.11_163_1_constrained in step 4\n",
      "  Saved 88 particles for 105_Sep12_18.54.50_185_1_constrained in step 4\n",
      "  Saved 120 particles for 97_Sep12_18.28.38_211_1_constrained in step 4\n",
      "  Saved 17 particles for 147_Sep13_11.30.28_255_1_constrained in step 4\n",
      "  Saved 193 particles for 154_Sep13_11.39.01_265_1_constrained in step 4\n",
      "  Saved 183 particles for 111_Sep12_19.09.17_193_1_constrained in step 4\n",
      "  Saved 79 particles for 98_Sep12_18.32.40_213_1_constrained in step 4\n",
      "  Saved 107 particles for 153_Sep13_11.46.34_261_1_constrained in step 4\n",
      "  Saved 24 particles for 146_Sep13_10.20.11_249_1_constrained in step 4\n",
      "  Saved 33 particles for 131_Sep13_10.52.39_221_1_constrained in step 4\n",
      "  Saved 141 particles for 147_Sep13_11.27.34_253_1_constrained in step 4\n",
      "  Saved 14 particles for 135_Sep13_10.23.29_229_1_constrained in step 4\n",
      "  Saved 126 particles for 93_Sep12_18.05.02_201_1_constrained in step 4\n",
      "  Saved 1 particles for 82_Sep12_16.22.05_175_1_constrained in step 4\n",
      "Saved 169 final particles for 158_Sep13_13.03.40_267_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 242 final particles for 140_Sep13_11.09.36_237_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 57 final particles for 113_Sep12_19.20.31_197_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 246 final particles for 151_Sep13_11.42.25_259_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 243 final particles for 142_Sep13_11.01.22_241_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 158 final particles for 79_Sep12_16.03.49_167_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 11 final particles for 100_Sep12_18.37.00_181_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 211 final particles for 163_Sep13_13.21.12_279_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 210 final particles for 153_Sep13_11.48.36_263_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 289 final particles for 141_Sep13_10.57.04_239_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 2 final particles for 82_Sep12_16.17.55_173_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 62 final particles for 128_Sep13_10.30.35_215_2_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 375 final particles for 138_Sep13_11.05.15_233_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 203 final particles for 144_Sep13_11.20.47_245_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 197 final particles for 94_Sep12_18.09.52_203_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 126 final particles for 95_Sep12_18.14.14_205_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 159 final particles for 147_Sep13_11.25.08_251_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 316 final particles for 139_Sep13_11.07.13_235_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 68 final particles for 150_Sep13_11.35.16_257_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 141 final particles for 143_Sep13_11.17.52_243_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 221 final particles for 95_Sep12_18.17.26_207_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 145 final particles for 77_Sep12_15.49.11_163_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 113 final particles for 105_Sep12_18.54.50_185_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 136 final particles for 97_Sep12_18.28.38_211_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 25 final particles for 147_Sep13_11.30.28_255_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 244 final particles for 154_Sep13_11.39.01_265_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 208 final particles for 111_Sep12_19.09.17_193_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 121 final particles for 98_Sep12_18.32.40_213_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 168 final particles for 153_Sep13_11.46.34_261_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 34 final particles for 146_Sep13_10.20.11_249_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 37 final particles for 131_Sep13_10.52.39_221_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 163 final particles for 147_Sep13_11.27.34_253_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "Saved 18 final particles for 135_Sep13_10.23.29_229_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 139 final particles for 93_Sep12_18.05.02_201_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 1 final particles for 82_Sep12_16.22.05_175_1_constrained (threshold: 5.562600243538888, correlations: 376245)\n",
      "Saved 1 final particles for 161_Sep13_13.14.49_275_1_constrained (threshold: 5.503402282492625, correlations: 268434)\n",
      "\n",
      "Processing complete. Final results saved to results_all_steps_4/final_results\n",
      "Total particles across all micrographs: 5259\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Processes results files from multiple directories sequentially.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import (\n",
    "    erfcinv,  # Required for the gaussian_noise_zscore_cutoff function\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.argv = [\"process_sequential_results.py\", \"results_constrained_step1\", \"results_constrained_step3\", \"results_constrained_step4\", \"results_constrained_step5\", \"--output\", \"results_all_steps_4\"]\n",
    "\n",
    "\n",
    "def gaussian_noise_zscore_cutoff(num_ccg: int, false_positives: float = 0.005) -> float:\n",
    "    \"\"\"Determines the z-score cutoff based on Gaussian noise model and number of pixels.\n",
    "\n",
    "    NOTE: This procedure assumes that the z-scores (normalized maximum intensity\n",
    "    projections) are distributed according to a standard normal distribution. Here,\n",
    "    this model is used to find the cutoff value such that there is at most\n",
    "    'false_positives' number of false positives in all of the pixels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_ccg : int\n",
    "        Total number of cross-correlograms calculated during template matching. Product\n",
    "        of the number of pixels, number of defocus values, and number of orientations.\n",
    "    false_positives : float, optional\n",
    "        Number of false positives to allow in the image (over all pixels). Default is\n",
    "        0.005 which corresponds to 0.5% false-positives.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Z-score cutoff.\n",
    "    \"\"\"\n",
    "    tmp = erfcinv(2.0 * false_positives / num_ccg)\n",
    "    tmp *= np.sqrt(2.0)\n",
    "\n",
    "    return float(tmp)\n",
    "\n",
    "\n",
    "def get_micrograph_id(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract micrograph ID from filename.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Filename to extract micrograph ID from\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    micrograph_id : str\n",
    "        Micrograph ID\n",
    "    \"\"\"\n",
    "    base_name = os.path.basename(filename)\n",
    "    # Extract the part before _results.csv\n",
    "    parts = base_name.split(\"_results.csv\")[0]\n",
    "    return parts\n",
    "\n",
    "\n",
    "def process_directories_sequentially(\n",
    "    directory_list: list[str],\n",
    "    output_base_dir: str,\n",
    "    false_positive_rate: float = 0.005,\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process directories sequentially.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory_list : list\n",
    "        Ordered list of directories to process\n",
    "    output_base_dir : str\n",
    "        Base directory to store output files\n",
    "    false_positive_rate : float\n",
    "        False positive rate to use for threshold calculation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_particles : dict\n",
    "        Dictionary of micrograph IDs as keys and df as values\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "    # Dictionary to store particles from all steps\n",
    "    # Key: micrograph_id\n",
    "    # Value: DataFrame of particles\n",
    "    all_particles = {}\n",
    "\n",
    "    # Dictionary to track which particles were found in which step\n",
    "    # Key: (micrograph_id, particle_index)\n",
    "    # Value: last step where this particle was found\n",
    "    particle_step_map = {}\n",
    "\n",
    "    # Dictionary to track total correlations per micrograph\n",
    "    # Key: micrograph_id\n",
    "    # Value: total correlations for this micrograph across all steps\n",
    "    micrograph_correlations = defaultdict(int)\n",
    "\n",
    "    # Dictionary to track thresholds per micrograph\n",
    "    # Key: micrograph_id\n",
    "    # Value: threshold for this micrograph in the current step\n",
    "    micrograph_thresholds = {}\n",
    "\n",
    "    # Process each directory in order\n",
    "    for step_idx, directory in enumerate(directory_list):\n",
    "        step_num = step_idx + 1\n",
    "        print(f\"\\nProcessing Step {step_num}: {directory}\")\n",
    "\n",
    "        # Create step output directory\n",
    "        step_output_dir = os.path.join(output_base_dir, f\"step_{step_num}\")\n",
    "        os.makedirs(step_output_dir, exist_ok=True)\n",
    "\n",
    "        # Find all results.csv files in the directory\n",
    "        results_files = glob.glob(\n",
    "            os.path.join(directory, \"**\", \"*_results.csv\"), recursive=True\n",
    "        )\n",
    "\n",
    "        if not results_files:\n",
    "            print(f\"  Warning: No results files found in {directory}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  Found {len(results_files)} results files\")\n",
    "\n",
    "        # Dictionary to store parameters from each micrograph\n",
    "        step_micrograph_parameters = {}\n",
    "\n",
    "        # First, find and read all parameters files to update correlation counts\n",
    "        for results_file in results_files:\n",
    "            micrograph_id = get_micrograph_id(results_file)\n",
    "            params_file = results_file.replace(\n",
    "                \"_results.csv\", \"_results_parameters.csv\"\n",
    "            )\n",
    "\n",
    "            if os.path.exists(params_file):\n",
    "                try:\n",
    "                    # Read the parameters file\n",
    "                    params_df = pd.read_csv(params_file)\n",
    "                    if not params_df.empty:\n",
    "                        step_micrograph_parameters[micrograph_id] = params_df.iloc[0]\n",
    "\n",
    "                        # Add to total correlations for this micrograph\n",
    "                        if \"num_correlations\" in params_df.columns:\n",
    "                            correlations = int(params_df.iloc[0][\"num_correlations\"])\n",
    "                            micrograph_correlations[micrograph_id] += correlations\n",
    "                            print(\n",
    "                                f\"  {micrograph_id}: Added {correlations} correlations \"\n",
    "                                f\"(total: {micrograph_correlations[micrograph_id]})\"\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error reading parameters file {params_file}: {e}\")\n",
    "            else:\n",
    "                print(f\"  Warning: Parameters file not found for {results_file}\")\n",
    "\n",
    "        # Calculate threshold for each micrograph based on its cumulative correlations\n",
    "        for micrograph_id, total_correlations in micrograph_correlations.items():\n",
    "            threshold = gaussian_noise_zscore_cutoff(\n",
    "                total_correlations, false_positive_rate\n",
    "            )\n",
    "            micrograph_thresholds[micrograph_id] = threshold\n",
    "            print(\n",
    "                f\"  Threshold for {micrograph_id} in step {step_num}: {threshold:.4f} \"\n",
    "                f\"(based on {total_correlations} total correlations)\"\n",
    "            )\n",
    "\n",
    "        # Process each results file\n",
    "        for results_file in results_files:\n",
    "            micrograph_id = get_micrograph_id(results_file)\n",
    "\n",
    "            try:\n",
    "                # Read the results file\n",
    "                results_df = pd.read_csv(results_file)\n",
    "\n",
    "                if results_df.empty:\n",
    "                    print(f\"  Warning: Empty results file {results_file}\")\n",
    "                    continue\n",
    "\n",
    "                # Get the threshold for this micrograph\n",
    "                if micrograph_id not in micrograph_thresholds:\n",
    "                    print(\n",
    "                        f\"  Warning: No correlation information for {micrograph_id}, \"\n",
    "                        \"using default threshold\"\n",
    "                    )\n",
    "                    # Try to use correlations from the current step's parameters file\n",
    "                    if (\n",
    "                        micrograph_id in step_micrograph_parameters\n",
    "                        and \"num_correlations\"\n",
    "                        in step_micrograph_parameters[micrograph_id]\n",
    "                    ):\n",
    "                        correlations = int(\n",
    "                            step_micrograph_parameters[micrograph_id][\n",
    "                                \"num_correlations\"\n",
    "                            ]\n",
    "                        )\n",
    "                        micrograph_correlations[micrograph_id] = correlations\n",
    "                        threshold = gaussian_noise_zscore_cutoff(\n",
    "                            correlations, false_positive_rate\n",
    "                        )\n",
    "                        micrograph_thresholds[micrograph_id] = threshold\n",
    "                        print(\n",
    "                            f\"  Using threshold {threshold:.4f} for {micrograph_id} \"\n",
    "                            f\"based on {correlations} correlations\"\n",
    "                        )\n",
    "                    else:\n",
    "                        # If no information at all, use the median of other thresholds\n",
    "                        # or a reasonable default\n",
    "                        if micrograph_thresholds:\n",
    "                            threshold = np.median(list(micrograph_thresholds.values()))\n",
    "                            print(\n",
    "                                f\"  Using median threshold {threshold:.4f} for \"\n",
    "                                f\"{micrograph_id}\"\n",
    "                            )\n",
    "                        else:\n",
    "                            #  Default if no other information is available\n",
    "                            threshold = 5.0\n",
    "                            print(\n",
    "                                f\"  Using default threshold {threshold:.4f} for \"\n",
    "                                f\"{micrograph_id}\"\n",
    "                            )\n",
    "                        micrograph_thresholds[micrograph_id] = threshold\n",
    "                else:\n",
    "                    threshold = micrograph_thresholds[micrograph_id]\n",
    "\n",
    "                # Check if refined_scaled_mip column exists\n",
    "                if \"refined_scaled_mip\" not in results_df.columns:\n",
    "                    print(\n",
    "                        f\" Warning: refined_scaled_mip not found in {results_file},\"\n",
    "                        \" using mip instead\"\n",
    "                    )\n",
    "                    compare_col = \"scaled_mip\"\n",
    "                else:\n",
    "                    compare_col = \"refined_scaled_mip\"\n",
    "\n",
    "                # Filter particles above threshold using the appropriate column\n",
    "                above_threshold_df = results_df[\n",
    "                    results_df[compare_col] > threshold\n",
    "                ].copy()\n",
    "\n",
    "                if above_threshold_df.empty:\n",
    "                    print(f\"  No particles above threshold in {results_file}\")\n",
    "                    continue\n",
    "\n",
    "                # Print stats\n",
    "                print(\n",
    "                    f\"{micrograph_id}: {len(above_threshold_df)} of {len(results_df)}\"\n",
    "                    f\" particles above threshold (using {compare_col})\"\n",
    "                )\n",
    "\n",
    "                # Add a step column to track which step this is from\n",
    "                above_threshold_df[\"step\"] = step_num\n",
    "\n",
    "                # If this is the first step, just add all particles above threshold\n",
    "                if step_num == 1:\n",
    "                    all_particles[micrograph_id] = above_threshold_df\n",
    "\n",
    "                    # Update particle step map\n",
    "                    for idx in above_threshold_df[\"particle_index\"]:\n",
    "                        particle_step_map[(micrograph_id, idx)] = step_num\n",
    "                else:\n",
    "                    # If this micrograph was not seen before, add all particles\n",
    "                    if micrograph_id not in all_particles:\n",
    "                        all_particles[micrograph_id] = above_threshold_df\n",
    "\n",
    "                        # Update particle step map\n",
    "                        for idx in above_threshold_df[\"particle_index\"]:\n",
    "                            particle_step_map[(micrograph_id, idx)] = step_num\n",
    "                    else:\n",
    "                        # For existing micrographs, handle particles differently\n",
    "                        existing_df = all_particles[micrograph_id]\n",
    "\n",
    "                        # Create a new DataFrame to store updated particles\n",
    "                        updated_df = existing_df.copy()\n",
    "\n",
    "                        # For each particle in the new results\n",
    "                        for _, particle in above_threshold_df.iterrows():\n",
    "                            particle_idx = particle[\"particle_index\"]\n",
    "\n",
    "                            # Check if this particle exists previously\n",
    "                            existing_particle = existing_df[\n",
    "                                existing_df[\"particle_index\"] == particle_idx\n",
    "                            ]\n",
    "\n",
    "                            if len(existing_particle) > 0:\n",
    "                                # Particle exists, update parameters\n",
    "                                # Find the index in the updated_df\n",
    "                                idx_to_update = updated_df.index[\n",
    "                                    updated_df[\"particle_index\"] == particle_idx\n",
    "                                ].tolist()[0]\n",
    "\n",
    "                                # Check if original offset columns exist\n",
    "                                offset_cols = [\n",
    "                                    \"original_offset_phi\",\n",
    "                                    \"original_offset_theta\",\n",
    "                                    \"original_offset_psi\",\n",
    "                                ]\n",
    "\n",
    "                                # Add original offset columns from step 1\n",
    "                                for col in offset_cols:\n",
    "                                    if col not in updated_df.columns:\n",
    "                                        updated_df[col] = 0.0\n",
    "\n",
    "                                # Add offset values from current step\n",
    "                                for col in offset_cols:\n",
    "                                    # Add particle's offset to existing offset\n",
    "                                    if col in particle and pd.notna(particle[col]):\n",
    "                                        updated_df.at[idx_to_update, col] += particle[\n",
    "                                            col\n",
    "                                        ]\n",
    "\n",
    "                                # Update other parameters\n",
    "                                for col in particle.index:\n",
    "                                    if col not in offset_cols and pd.notna(\n",
    "                                        particle[col]\n",
    "                                    ):\n",
    "                                        updated_df.at[idx_to_update, col] = particle[\n",
    "                                            col\n",
    "                                        ]\n",
    "\n",
    "                                # Update step\n",
    "                                updated_df.at[idx_to_update, \"step\"] = step_num\n",
    "                                particle_step_map[(micrograph_id, particle_idx)] = (\n",
    "                                    step_num\n",
    "                                )\n",
    "                            else:\n",
    "                                # New particle, add it to the DataFrame\n",
    "                                updated_df = pd.concat(\n",
    "                                    [updated_df, pd.DataFrame([particle])],\n",
    "                                    ignore_index=True,\n",
    "                                )\n",
    "                                particle_step_map[(micrograph_id, particle_idx)] = (\n",
    "                                    step_num\n",
    "                                )\n",
    "\n",
    "                        # Update the all_particles dictionary\n",
    "                        all_particles[micrograph_id] = updated_df\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing results file {results_file}: {e}\")\n",
    "\n",
    "        # Save intermediate results for this step\n",
    "        for micrograph_id, particles_df in all_particles.items():\n",
    "            # Only save particles found or updated in this step\n",
    "            step_particles = particles_df[particles_df[\"step\"] == step_num]\n",
    "\n",
    "            if not step_particles.empty:\n",
    "                output_file = os.path.join(\n",
    "                    step_output_dir, f\"{micrograph_id}_results_above_threshold.csv\"\n",
    "                )\n",
    "                step_particles.to_csv(output_file, index=False)\n",
    "                print(\n",
    "                    f\"  Saved {len(step_particles)} particles for {micrograph_id} \"\n",
    "                    f\"in step {step_num}\"\n",
    "                )\n",
    "\n",
    "    # Save final results after all steps\n",
    "    final_output_dir = os.path.join(output_base_dir, \"final_results\")\n",
    "    os.makedirs(final_output_dir, exist_ok=True)\n",
    "\n",
    "    # Save summary of total particles per micrograph\n",
    "    summary_data = []\n",
    "\n",
    "    for micrograph_id, particles_df in all_particles.items():\n",
    "        output_file = os.path.join(\n",
    "            final_output_dir, f\"{micrograph_id}_results_above_threshold.csv\"\n",
    "        )\n",
    "        particles_df.to_csv(output_file, index=False)\n",
    "\n",
    "        # Get the final threshold for this micrograph\n",
    "        final_threshold = micrograph_thresholds.get(micrograph_id, \"N/A\")\n",
    "        total_correlations = micrograph_correlations.get(micrograph_id, 0)\n",
    "\n",
    "        # Create summary data\n",
    "        n_particles = len(particles_df)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"micrograph_id\": micrograph_id,\n",
    "                \"total_particles\": n_particles,\n",
    "                \"total_correlations\": total_correlations,\n",
    "                \"final_threshold\": final_threshold,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Saved {n_particles} final particles for {micrograph_id} \"\n",
    "            f\"(threshold: {final_threshold}, correlations: {total_correlations})\"\n",
    "        )\n",
    "\n",
    "    # Save summary\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(\n",
    "        os.path.join(final_output_dir, \"processing_summary.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\nProcessing complete. Final results saved to {final_output_dir}\")\n",
    "\n",
    "    # Print total particles\n",
    "    total_particles = sum(len(df) for df in all_particles.values())\n",
    "    print(f\"Total particles across all micrographs: {total_particles}\")\n",
    "\n",
    "    return all_particles\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main function to process results files sequentially.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Process results files from multiple directories sequentially\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"directories\", nargs=\"+\", help=\"Ordered list of directories to process\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\", \"-o\", required=True, help=\"Output directory for results\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--false-positive-rate\",\n",
    "        \"-f\",\n",
    "        type=float,\n",
    "        default=0.005,\n",
    "        help=\"False positive rate for threshold calculation (default: 0.005)\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Check if all directories exist\n",
    "    for directory in args.directories:\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"Error: Directory {directory} does not exist!\")\n",
    "            return\n",
    "\n",
    "    # Process directories\n",
    "    process_directories_sequentially(\n",
    "        args.directories, args.output, args.false_positive_rate\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leopard-em",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
